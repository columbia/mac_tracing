\subsection{Event Graph Basics}\label{subsec:eventgraph}

In this section, we first describe the basics of \xxx event graphs
(\S\ref{subsec:eventgraph}). 
To construct event graphs, \xxx collects three categories of events in its
systems-wide event logs.  The first category contains semantic events,
such as system calls, call stacks collected when certain operations such
as macOS message operations are run, and user actions such as key presses.
These events indicate what the developer intents might be, and are stored
as contents in each vertex in the event graph.  They are primarily for
providing information to user during diagnosis and for finding similar
vertices (\S\ref{sec:overview}).

The second category of events are boundary events that mark the beginning
and ending of execution segments or vertices in the graph.  \xxx handles
common callback invocations such as \vv{dispatch\_invoke} and
\vv{runloop\_invoke} and mark their entry and return as boundaries.

The third category of events are for forming edges in the graph.  For
instance, an operation that installs a callback is connected to the
execution of the callback.  A message send is connected to a message
receive.  The arming of a timer is connected to the execution of the timer
callback.  A unique design in \xxx is to trace general wake-up and wait
operations inside the kernel to ensure coverage across many diverse
user-level, possibly custom wake-up and wait operations because their
implementations almost always use kernel wake-up wand wait.  This approach
necessarily includes spurious edges in the graph, including those due to
mutual exclusion; \xxx handles them by querying the user when it
encounters a vertex with multiple incoming causal edges during diagnosis
(see \S\ref{sec:overview}).  We also observed that a waiting kernel thread
is frequently woken up to perform tasks such as interrupt handling and
scheduler maintenance; \xxx recognizes them and culls them out from the
graph automatically.

%% \xxx constructs dependency graphs with the events from tracing logs. Tracing
%% logs contain sequence of events per thread. Each event stands for an execution
%% step in a thread. They are grouped into nodes and IPCs, asynchronouns calls and
%% thread wakeups serve as edges; some edges can be inside a single thread.

%% The events traced in \xxx are carefully selected for three main purposes:
%% preserve semantics for the node, indentify node boundaries inside a thread,
%% and provide connections between nodes. We classify them into three categories:
%% semantics events, boundary events and connection events, as listed in
%% Table~\ref{table:event_types}.

%% \begin{table}[ht]
%% \begin{threeparttable}
%%   \centering
%%   \begin{tabularx}{\columnwidth}{|X|X|}
%%   	\hline
%%     \textbf{Event Type} & \textbf{Event Categories}\\
%% 	\hline
%% 	\hline
%% 		System\_call & Semantics\\ \hline
%% 		Back\_trace & Semantics\\ \hline
%% 		NSApp\_event\tnote{1} & Semantics \\ \hline
%% 		Wait & Semantics, Boundary \\ \hline
%% 		Interrupts & Boundary \\ \hline
%% 		Sharetime\_maintenance\tnote{2} & Boundary \\ \hline
%% 		Dispatch\_invoke\tnote{3} & Boundary \\ \hline
%% 		Runloop\_invoke & Boundary \\ \hline
%% 		Mach\_message & Boundary, Connection\\ \hline
%% 		Wake\_up & Connection \\ \hline
%% 		Timer & Connection \\ \hline
%% 		Dispatch\_enqueue & Connection \\ \hline
%% 		Runloop\_submit & Connection \\ \hline
%% 		Share\_flag\_read & Connection \\ \hline
%% 		Share\_flag\_write & Connection \\ \hline
%%   \end{tabularx}

%% 	\begin{tablenotes}
%% 		\footnotesize
%% 		\item [1] User input events dispatched to the Application.
%% 		\item [2] Kernel invoked a routine to update timeshare quota. 
%% 		\item [3] Invoke callback function for works from dispatch queue.
%% 	\end{tablenotes}
%%  \end{threeparttable}
%% \caption{Event Type Categories. }
%% \label{table:event_types}
%% \end{table}

Compared to tools such as \spindump that capture only the current
system state, event graphs capture the causal path of events, enabling
users to trace across threads and process to events happened in the past
(hence cannot be captured by \spindump) that explain present
anomalies.

%% Given the prevalent of multi-threading and multi-processing programs, bugs are
%% much more complicated. The long opening bugs are usually have several threads
%% involve, even across process boundaries. As an example, the always timeout on
%% particular synchronization primitive in one thread usually need to trace back to
%% find the other thread that was responsible for signal the primitive. Compared
%% to the existing debugging tools like lldb and spindump, the dependency graph is
%% useful in that 1) it provides thread relationships all over the system across
%% process boundary and timing boundary and 2) it records execution history for an
%% input event before users capture hangs with their eyes.
