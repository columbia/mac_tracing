\subsection{\xxx Event Graph}\label{subsec:eventgraph}

Compared to prior tools, \xxx differs in three key design choices of its
event graphs.  First, since event graphs are inherently inaccurate, \xxx
embraces inaccuracies and mitigates them.  For instance, instead of
assuming a small fixed set of communication idioms, \xxx is tracing
general wake-up and wait operations inside the kernel to ensure coverage
across diverse user-level custom synchronization primitives because their
implementations almost always use kernel wake-up and wait.  This approach
necessarily includes spurious edges in the graph, including those due to
mutual exclusion and context switch by interrupts.  \xxx filters some of
these edges automatically such as those added due to interrupts because
they do not reflect application intents.  It applies several effective
heuristics to further refine them (see \S\ref{sec:inaccuracy}), and
queries the user interactively when needed to resolve ambiguity.

Second, \xxx differentiates strong and weak causal edges in its
graph. When there is strong evidence of causality, such as callback
dispatch, it adds strong edges.  When it adds edges heuristically to
mitigate inaccuracies (see \S\ref{sec:inaccuracy}), it marks them weak.
Its diagnosis algorithm prefers strong over weak edges, and follows weak
edges only when no strong edges present.  In case of ambiguity, it queries
the user for resolution.

Third, \xxx stores extra semantical information in the execution segments
of the graphs, including system calls, call stacks when certain operations
such as \vv{mach\_msg} are running, and user actions such as key
presses. These events enable \xxx to find potential correct vs buggy
executions for comparative diagnosis, and also provide users necessary
information for interactive debugging.

%% The event graph is a generalized control-flow graph which includes inter-thread
%% and inter-process dependencies. To construct event graphs, \xxx collects three
%% categories of events in its systems-wide event logs. The first category of
%% events are boundary events that mark the beginning and ending of execution
%% segments. \xxx handles common callbacks, such as \vv{dispatch\_client\_callout}
%% and \vv{CFRunLoopDoBlocks}, and mark their entry and return as boundaries.
%% Every execution segment corresponds to a vertex in the event graph. The second
%% category contains semantic events, including system calls, call stacks when
%% certain operations such as \vv{mach\_msg} are running, and user actions such as
%% key presses. These events are stored as contents in the vertex and primarily for
%% providing information to user during diagnosis. The third category of events are
%% communication events for forming edges in the graph. For instance, an operation
%% that installs a callback is connected to the invocation of the callback.
%% %They help users diagnose bugs across thread/process boundaries.

%% %A message send is connected to a message receive. The arming of a
%% %timer is connected to the processing of the timer callback. 

%% A unique design in \xxx is tracing general wake-up and wait operations inside
%% the kernel to ensure coverage across diverse user-level custom
%% synchronization primitive because their implementations almost always
%% use kernel wake-up and wait. \xxx includes the wake-up in the category of
%% communication event, and classifies the wait into the category of boundary
%% event. This approach necessarily includes spurious edges in the graph, including
%% those due to mutual exclusion and context switch by interrupts; \xxx handles
%% them by querying the user when it encounters a vertex with multiple incoming
%% causal edges during diagnosis (see \S\ref{subsec:overflow}). We also observed
%% that a waiting kernel thread is frequently woken up to perform tasks such as
%% timer firing signal and scheduler maintenance; \xxx recognizes them and culls
%% them out from the graph automatically.

%% Compared to tools such as \spindump that capture only the current system state,
%% event graphs capture the causal path of events, enabling users to trace across
%% threads and processes to events happened in the past (hence cannot be captured by
%% \spindump) that explain present anomalies. Therefore \xxx can report
%% root causes such as dead locks due to design flaws.

%% Given the prevalent of multi-threading and multi-processing programs, bugs are
%% much more complicated. The long opening bugs are usually have several threads
%% involve, even across process boundaries. As an example, the always timeout on
%% particular synchronization primitive in one thread usually need to trace back to
%% find the other thread that was responsible for signal the primitive. Compared
%% to the existing debugging tools like lldb and spindump, the dependency graph is
%% useful in that 1) it provides thread relationships all over the system across
%% process boundary and timing boundary and 2) it records execution history for an
%% input event before users capture hangs with their eyes.

