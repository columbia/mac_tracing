\subsection{Event Graph Basics}\label{subsec:eventgraph}

The event graph is a generalized control-flow graph which includes inter-thread
and inter-process dependencies. To construct event graphs, \xxx collects three
categories of events in its systems-wide event logs. The first category of
events are boundary events that mark the beginning and ending of execution
segments. \xxx handles common callbacks, such as \vv{dispatch\_client\_callout}
and \vv{CFRunLoopDoBlocks}, and mark their entry and return as boundaries.
Every execution segment corresponds to a vertex in the event graph. The second
category contains semantic events, including system calls, call stacks when
certain operations such as \vv{mach\_msg} are running, and user actions such as
key presses. These events are stored as contents in the vertex and primarily for
providing information to user during diagnosis. The third category of events are
communication events for forming edges in the graph. For instance, an operation
that installs a callback is connected to the invocation of the callback. They
help users diagnose bugs across thread/process boundaries.

%A message send is connected to a message receive. The arming of a
%timer is connected to the processing of the timer callback. 

A unique design in \xxx is to trace general wake-up and wait operations inside
the kernel to ensure coverage across many diverse user-level, possibly custom
wake-up and wait operations because their implementations almost always use
kernel wake-up and wait. This approach necessarily includes spurious edges
in the graph, including those due to mutual exclusion and context switch by
interrupts; \xxx handles them by querying the user when it encounters a vertex
with multiple incoming causal edges during diagnosis (see \S\ref{subsec:overflow}).
We also observed that a waiting kernel thread is frequently woken up to perform
tasks such as timer firing signal and scheduler maintenance; \xxx recognizes
them and culls them out from the graph automatically.

Compared to tools such as \spindump that capture only the current system state,
event graphs capture the causal path of events, enabling users to trace across
threads and processes to events happened in the past (hence cannot be captured by
\spindump) that explain present anomalies. Therefore \xxx can report
root causes such as dead locks due to design flaws.

%% Given the prevalent of multi-threading and multi-processing programs, bugs are
%% much more complicated. The long opening bugs are usually have several threads
%% involve, even across process boundaries. As an example, the always timeout on
%% particular synchronization primitive in one thread usually need to trace back to
%% find the other thread that was responsible for signal the primitive. Compared
%% to the existing debugging tools like lldb and spindump, the dependency graph is
%% useful in that 1) it provides thread relationships all over the system across
%% process boundary and timing boundary and 2) it records execution history for an
%% input event before users capture hangs with their eyes.

