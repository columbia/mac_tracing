\section{Overview} \label{sec:overview}

%\subsection{Interactive Diagnosis} \label{subsec:debug}

In this section, we describe the steps a user takes to investigate a
performance anomaly with \xxx. We assume the user has a program to execute and
a buggy test case that can be reliably reproduced; if possible, the user can
also provide a similar test case which executes normally for comparison
purposes. The goal of \xxx is to discover a sequence of user-interface (UI)
actions that trigger the buggy test case, and a detailed execution trace for
these events. In the cases we consider, the flow of information across threads
and processes is essential to discovering the system state that leads to a
performance bug. \xxx recovers UI actions from logged data rather than being
told the actions that a user performs, because not all of them may be relevant
to the true bug.

Central to our system is our \emph{event graph}, a generalized control-flow
graph which includes inter-thread and inter-process dependencies. All diagnoses
and inferences are performed within this graph, in a semi-automated fashion:
\xxx performs searches and subgraph matching to trace logical events as they
flow through the system, and the user can interactively query this graph to
understand the problem or provide guidance to \xxx. The event graph is
described in further detail in \S\ref{subsec:eventgraph}. Next, we describe the
graph operations \xxx performs leading to a diagnosis.

\subsection{Graph Exploration and Diagnosis} \label{subsec:debug}

The exact operations performed on the graph depend on the bug under
investigation. Consider a common performance bug on MacOS, the \emph{spinning
cursor}, which indicates the current application's main thread has not
processed any UI events for over two seconds. Upon examining what the main
thread is actually doing, the user may encounter three potential cases.

First, the thread may be busy performing lengthy CPU operations (which take
longer than two seconds). In this case, \xxx will examine the stack trace and
find a sequence of events that led to the current CPU processing. If more
specific tracing is required, the user can rerun the program with more
heavyweight instrumentation enabled for any portion of the code's execution,
gathering a precise sequence of calls or even instructions executed.

Second, the thread may be in a blocking wait, in which case \xxx tries to
automatically determine which event would normally wake up the thread. The
system does so by diffing the current subgraph with other instances in the
recorded execution (assuming the user has carried out a typical baseline test
case before triggering the bug). Essentially, \xxx traces blocking events and
messages backwards through history until relevant UI events are found. The user
may manually intervene at different stages to guide the system towards the most
relevant portions of execution, leveraging hypotheses or expert knowledge as to
why a hang may occur. The full diff algorithm is described in \S XXX.

Third, the main thread may be in a yield loop, which is highly indicative it
is waiting on a data flag (\eg, ``while(!done)
thread\_switch();'').
To discover a data flag, the user re-runs the application
with \xxx to collect instruction traces of the concurrent event in both the
normal and spinning cases and detects where the control flow diverges.  She
then reruns the application with \xxx to collect register values for the basic
blocks before the divergence and uncovers the address of the data flag.  She
then configures \xxx to log accesses to the flag during system-wide tracing.
Finally, she can recursively apply \xxx to further diagnose ``the culprit of
the culprit''.

Based on our results and experience, the first case is the most common, but the
second and third represent more severe bugs. Long-running CPU operations tend
to be more straightforward to diagnose with existing tools. Blocking or
yielding cases involve multiple threads and processes, and are extremely hard
to understand and fix even for the application's original developers.
Therefore, issues remain unaddressed for years and significantly impact the
user experience.

%After \xxx builds the event graph, a user can interactively query this graph
%for diagnosis.  For instance, consider a spinning cursor in MacOS which
%indicates the current application's main thread has not processed any UI events
%for over two seconds.  The user can query \xxx to find the ongoing event in the
%application's main thread concurrent to the display of the spinning cursor.
%Depending on the type of the event, she can proceed in three directions.

%First, if the concurrent event is a busy operation that occupies the CPU, she
%has found the cause of the spinning because a busy main thread cannot process
%UI events.  She can examine the event's lightweight call stack.  If it does not
%provide enough details, she can rerun the application and use \xxx's
%fine-grained debugging tool to obtain a more complete call stack, the addresses
%of the instructions executed, and parameters and return values of call
%instructions.  In general, she can increase debugging details for any event,
%not just a busy event.

%Second, if the concurrent event is a blocking wait, she runs \xxx to locate
%another event in the graph that causes the wake-up to arrive late. \xxx does so
%using the following idea.  In the normal case, there must be a path of wake-up
%edges that leads to the blocking wait, so the main thread starts running again.
%In the spinning case, somewhere along the path, a thread's wake-up is missing,
%so this thread is the culprit.  Mechanically, \xxx first searches the graph to
%find a similar wait that does not cause a spinning cursor.  If there are
%multiple nodes similar to the wait, \xxx asks the user to pick one.  It then
%slices the graph backwards to find the wake-up path.  If an event has exactly
%one inter-thread edge or only an intra-thread edge, it follows the edge.
%Otherwise, the event must have two or more inter-thread edges, and \xxx
%consults the user to pick one to follow.  Given the path, \xxx examples the
%threads in the path one by one, and returns the thread whose wake-up is missing
%in the spinning case.
%
%Third, if the concurrent event is a thread yield, it is highly indicative that
%the main thread is waiting on a data flag (\eg, ``while(!done)
%thread\_switch();'').  To discover a data flag, the user reruns the application
%with \xxx to collect instruction traces of the concurrent event in both the
%normal and spinning cases and detects where the control flow diverges.  She
%then reruns the application with \xxx to collect register values for the basic
%blocks before the divergence and uncovers the address of the data flag.  She
%then configures \xxx to log accesses to the flag during system-wide tracing.
%Finally, she can recursively apply \xxx to further diagnose ``the culprit of
%the culprit''. 
%
%Based on our results, the first type of spinning cursor is more common but the
%second and third types cause the most harm.  The reason is that the first type
%tends to be straightforward to diagnose, so they are fixed quickly.  The second
%and third types involve multiple threads, so they are extremely hard to
%understand and fix even for the application's own developers.  Therefore, they
%remain open for years and ruin many users' experiences.

\subsection{\xxx Algorithm}
\begin{algorithm}[h!]
    \caption{Main \xxx algorithm.}
    \label{alg:alg-main}
\begin{algorithmic}[1]
\Require{Program to run + buggy test case + (optional) similar baseline test case}
\Ensure{Sequence of actions or UI events which trigger the performance problem, and trace of execution across event handlers}
\Statex
\Function{\xxx Main}{}
\State {$TracingEvents$ $\gets$ Run program under \xxx, trigger baseline case (if possible) and then buggy test case}
\State {$HeuristicsSet$ $\gets$ $\{default$ $ $ $heuristics\}$}
\State {$ControlFlowGraph$ $\gets$ ComputeGraph($HeuristicsSet$, $TracingEvents$)}\label{recompute}
\State {$SpinningNode$ $\gets$ search $ControlFlowGraph$ with timestamp when spinning cursor displays}
\If {$SpinningNode$ $==$ $CPU busy execution(livelock)$}
	\State{run backward traversal of edges until UI event found}
\Else
	\State{$N_0$ $\gets$ $SpinningNode$}
	\State{$N_1$ $\gets$ FindSimilarNode($ControlFlowGraph$, $N_0$)}
	\State{call AssistedGraphDiff($ControlFlowGraph$, $N_0$, $N_1$)}
	\If {new\_hueristics were added}
		\State \Goto{recompute}
	\EndIf
\EndIf
\State{Return set of UI events and root cause related Nodes}
\EndFunction
\end{algorithmic}
\end{algorithm}

%%\begin{figure}[tb]
%%\footnotesize\begin{verbatim}
%%Main Algorithm:
%%    Input: Program to run + buggy test case
%%        + (optional) similar baseline test case
%%    Output: Sequence of actions or UI events which
%%        trigger the performance problem, and trace
%%        of execution across event handlers
%%
%%1. Run program under \xxx, trigger baseline case
%%    (if possible) and then buggy test case
%%2. Set heuristics = { default heuristics }
%%3. Compute graph using current heuristics with
%%    Algorithm ComputeControlFlowGraph
%%4.a. If the buggy case is busy executing code (livelock):
%%    run backward traversal of edges until UI event found
%%4.b. Else, given a hanging node N_1, use Algorithm
%%    FindSimilarNode to obtain an equivalent
%%    in the baseline test case,
%%    N_0 -> FindSimilarNode(N_1)
%%    5.a. Compare nodes N_0 and N_1, and automatically
%%        diff the two cases, moving through history
%%        semi-automatically with user input
%%	    until useful UI events are found
%%        [Algorithm AssistedGraphDiff]
%%    5.b. If new heuristics were added, go to step 3.
%%6. Return set of UI events found
%%\end{verbatim}
%%    \caption{Main \xxx algorithm.}
%%    \label{fig:alg-main}
%%\end{figure}


\paragraph{Main algorithm} Next, we describe \xxx's operation more precisely.
Algorithm~\ref{alg:alg-main} shows the steps a user takes to investigate a
performance anomaly with \xxx, given a buggy test case and (potentially) a
baseline test case for comparison purposes.

First (step 1) the user executes the program with \xxx instrumentation enabled,
running through the baseline test case (if applicable) and then the buggy test
case, capturing execution logs. Next (steps 2-3), \xxx computes a generalized
control-flow graph which includes inter-thread and inter-process dependencies;
this graph is described in further detail in \S\ref{subsec:eventgraph}. In step
4.a and 4.b, the goal is to track events and synchronization primitives
throughout the system to figure out the root cause, in terms of user input
events, which caused a hang or performance bug. We locate the node $N_1$ in our
graph where the performance bug is triggered. This node could be executing code
(step 4.a) or in a deadlock situation (step 4.b). The most common and most
complex situation is the latter, where we use a diff between the buggy test
case and a baseline test case to further diagnosis. We locate a node $N_0$ in
our graph that corresponds to the baseline version of the hanging node $N_1$
(see \S XXX), and diff the subgraphs to determine the differences between the
cases. This diffing technique, detailed in \S\ref{subsec:debug} below, is
semi-automated but can integrate user input at each stage to leverage
hypotheses or expert knowledge as to why a hang may occur.


\begin{algorithm}[h!]
    \caption{\xxx Assisted graph diff algorithm.}
    \label{alg:alg-graphdiff}
\begin{algorithmic}[1]
\Require{SpinningNode + BaselineNode + Graph}
\Ensure{Possible root cause of thread blocking}
\Statex
\Function{AssistedGraphDiff}{}
\State{$ResultSet$ $\gets$ $\{\}$}
\State{$SlicingPath$ $\gets$ BackwardSlicing on $BaseLineNode$}
\State{$T_spinning$ $\gets$ timestamp of spinning beachball for $SpinningNode$}
\For{$thread$ $\gets$ thread of $Node_i$ in $SlicingPath$}
	\State{$T_normal$ $\gets$ timestamp of last event in $Node_i$}
	\State{$Node_blocking$ $\gets$ $thread$ blocks during ($T_normal$, $T_spinning$)}
	\If {$Nodeblocking$  $\neq$ $NULL$}
		\State{put $Node_blocking$ into $ResultSet$}
	\EndIf
	\If {$UI Events$ in Main UI thread}
		\State{put $UI Event$ into $ResultSet$}
	\EndIf
\EndFor
\State{Return $ResultSet$}
\EndFunction
\end{algorithmic}
\end{algorithm}

%%\begin{figure}[tb]
%%\footnotesize\begin{verbatim}
%%Algorithm AssistedGraphDiff:
%%  Input: Backward slicing path from the baseline Node
%%          + Spinning Node + Graph
%%  Output: Possible root cause of thread blocking
%%  1. get timestamp of Spinning Node as t_spinning
%%  2. For every thread, node in the backware slicing path
%%     get timestamp of node in the baseline as t_normal
%%     check if thread block during (t_normal. t_spinning)
%%     if true, put the blocking node in the result set
%%  3. return the result set
%%\end{verbatim}
%%    \caption{\xxx Assisted graph diff algorithm.}
%%    \label{fig:alg-graphdiff}
%%\end{figure}


\paragraph{Graph diff algorithm}
We mentioned previously in \S\ref{subsec:debug} that \xxx explores the event
graph semi-automatically to narrow in on relevant nodes for diagnosis. This
full algorithm, which includes steps that accept manual user intervention, is
shown in Algorithm~\ref{alg:alg-graphdiff}.

\paragraph{Other algorithms}
More detailed algorithms that require no user intervention are described in later sections.
Specifically, ComputeControlGraph is in \S XXX and FindSimilarNode is in \S XXX.

%\begin{figure*}[tb]
%    \centering
%	\input{figures/overview_tikz.tex}
%    \caption{\xxx Work Flow}
%    \label{fig:argus-overview}
%\end{figure*}

%Figure~\ref{fig:argus-overview} shows \xxx's work flow, which consists of two
%phases. A user runs command ``\vv{\xxx start}'' to enter the system-wide tracing
%phase, within which \xxx logs events including system calls, inter-process
%communications (IPCs), and waits and wake-ups from all applications and daemons.
%Occasionally based on user configurations, it logs data flag accesses leveraging
%hardware watch-point registers. Each log entry includes a timestamp, the event
%type, key attributes of the event, and an optional lightweight call stack
%obtained by unwinding the user stack pointer. \xxx implements tracing by
%instrumenting core system libraries and a small portion of the operating system.
%The performance impact of this system-wide tracing is low because the logged
%events themselves are often expensive and require user-kernel crossings, masking
%the overhead of tracing.
%
%Whenever a user detects a performance issue such as a spinning cursor, she
%runs ``\vv{\xxx debug}'' to enter the interactive diagnosis phase. \xxx
%initializes this phase by constructing a causal graph from all logged events up
%to user-specified duration. Rather than requiring a precise application-specific
%user-written schema, \xxx leverages a simple system-wide schema we created
%to construct approximate graphs (\S\ref{subsec:eventgraph}), and relies on
%interactive user insights for diagnosis (\S\ref{subsec:debug}).

%ComputeControlGraph takes the heuristics set and the parsed trace
%events as input and produce a event graph as is described in
%Section \S\ref{subsec:eventgraph}. The algorithm is shown in
%Figure~\ref{fig:alg-graphcomputing}. The graph is subject to improve by rerun
%the computing process with new heuristics.

\begin{algorithm}[h!]
    \caption{\xxx Find similar node algorithm.}
    \label{fig:alg-findsimilarnode}
\begin{algorithmic}[1]
%%  Output: Similar node set
\Require{EventTypesSet + BlockingNode + Graph}
\Ensure{SimilarNodeSet}
\Statex
\Function{FindSimilarNode}{}
\State{$thread$ $\gets$ thread of $Node_blocking$}
\For {$Node_i$ in $thread$ other than $Node_blocking$}
	\State{$iter_i$ $\gets$ iterator of first event in $Node_i$}
	\State{$iter_b$ $\gets$ iterator of first event in $Node_blocking$}
	\While {$iter_b$ $\ne$ end \And $iter_i$ $\ne$ end}
		\While {$iter_i$ $\ne$ end \And $Event$($iter_i$) $\notin$ $EventTypesSet$}
			$iter_i$++
		\EndWhile
		\While {$iter_b$ $\ne$ end \And $Event$($iter_b$) $\notin$ $EventTypesSet$}
			$iter_b$++
		\EndWhile
		\If {$iter_b$ $\ne$ end \And $iter_i$ $\ne$ end}
			$if_eq$ $\gets$ CompareEvent($Event$($iter_b$), $Event$($iter_i$))
			\If {$if_eq$ $==$ False}
				\State{Break}
			\EndIf
		\EndIf
	\EndWhile
	\If {$iter_b$ $==$ end \And $iter_i$ $==$ end}
		\If {wresult of $Node_i$ $\neq$ wresult of $Node_blocking$ \Or timespan of $Node_blocking$ - timespan of $Node_i$ $\geq$ threshhold}
			\State{Put Node into $SimilarNodeSet$}
		\EndIf
	\EndIf
\EndFor
\EndFunction
\end{algorithmic}
\end{algorithm}

%%\begin{figure}[tb]
%%\footnotesize\begin{verbatim}
%%Algorithm FindSimilarNode:
%%  Input: Checking Events + CurrentNode + Graph
%%  Output: Similar node set
%%1. Get the thread id of CurrentNode
%%2. Iterate the node from the same thread N_i, 
%%   compare them to CurrentNode N_c:
%%   while event{iter_c} in N_c is not checking event
%%   and iter_c not reaches end
%%   	 iter_c++
%%   while event{iter_i} in N_i is checking event
%%   and iter_i not reaches end
%%     iter_i++
%%   if iter_c and iter_i do not reach end:
%%      Compare Event{iter_c} and Event{iter_i}:
%%        2.a wait events compares the wait resource
%%        2.b connection events compare their peer
%%        2.c system call events and user input events
%%            compare their syscallname and eventname
%%   if iter_c and iter_i both reach end:
%%      put N_i into the result set
%%3. filter the result set if:
%%   3.a if the wait events in the end of node
%%       have the same wait result
%%   3.b if the difference of node time span
%%       is within threshold
%%4. return the result set
%%\end{verbatim}
%%    \caption{\xxx Find similar node algorithm.}
%%    \label{fig:alg-findsimilarnode}
%%\end{figure}
%%
%FindSimilarNode in Figure~\ref{fig:alg-findsimilarnode} is meant to find nodes
%which have the same high level semantics but different execution results.
%For example, both of them are waiting on locks, but one returns successfully and the other not.
%\xxx assumes that high level statements are likely executed repeatedly in a process life cycle.
%In the algorithm, \xxx defines a subset of
%events that peserve semantics, which contains mach\_msg event, wait event,
%dispatch queue events, runloop events, breakpointer events, user input events
%and system call events.
%The comparison of events depends on their types.
%\xxx finds nodes that have different wait results from the spinning node by default.
%Depending on the report detail of the spinning node, user can
%change the comparing metrics.


\subsection{Chromium Spinning Cursor Example}

%   resolve symbol, save to log
%     search for set\_spinning
%     if found,
%       find main thread node at the time of set\_spinning
%     find fontd
%     manually check nodes in each thread immediately after the nodes in the slice (normal abnormal boundary)
%  output is a node, and a HTML dump of node and immediate predecessors and successors
% systems preference          
%  spinning node in UI thread is not waiting
%  so we look for messages, and find the diff

One of the authors experienced first-hand the aforementioned performance issue
in Chromium, an open-source browser engine that powers Google Chrome and,
starting recently, Microsoft Edge~\cite{chromiumurl}.  She tried to type in the
Chromium search box a Chinese word using SCIM, the default Chinese Input Method
Editor that ships with MacOS.  The browser appeared frozen and the spinning
cursor occurs for a few seconds.  Afterwards everything went back to normal.
This issue is reproducible and always ruins her experience, but it is quite
challenging to diagnose because two applications Chromium and SCIM and many
daemons ran and exchanged messages.  This issue was reported by other users for
other non-English input methods, too.

To diagnose this issue with \xxx, the author started system-wide tracing, and
then reproduced the spinning cursor with a Chinese search string typed via SCIM
while the page was loading. It produced normal cases for the very first few
characters, and the browser got blocked with the rest input as spinning cases.
The entire session took roughly five minutes.

She then ran \xxx to construct the event graph.  The graph had 2,749,628
vertexes and 3,606,657 edges, almost fully connected.  It spans across 17
applications; 109 daemons including \vv{fontd}, \vv{mdworker}, \vv{nsurlsessiond}
and helper tools by applications; 126 processes; 679 threads, and 829,287 IPC
messages.  Given the scale of the graph and the diverse communication patterns,
it would be extremely challenging for prior automated causal tracing
tools~\cite{aguilera2003performance, zhang2013panappticon, attariyan2012x,
cohen2004correlating} because they handle a fairly limited set of patterns.
Tools that require manual schema~\cite{barham2004using, reynolds2006pip}, would
be prohibitive because developers would have to provide schema for all involved
applications and daemons.

%%\begin{figure*}[p]
%%    \centering
%%	\input{figures/chromium_case_study_tikz.tex} 
%%    \caption{Chromium case study.}
%%    \label{fig:chromium-trace}
%%\end{figure*}

Next she ran \xxx to find the event in the main thread of the browser process.
\xxx returned a \vv{cv\_timed\_wait} event (Figure~\ref{fig:chromium-trace})
that blocked the main thread for a few seconds.  Inspection of the lightweight
call stack revealed that this wait happened within a call to
\vv{TextInputClientMac::GetFirstRectForRange}.  Without knowing the
application's semantics, she could not understand this method.  Thus she ran
\xxx to compare the spinning case to a normal case.  \xxx searched in the main
thread of the browser process for vertexes similar to this wait waiting
vertexes (contain \vv{write\_file, cv\_timed\_wait} in this case) similar to
this wait, found three, and confirmed with the user which one she wanted.

\xxx then found the normal-case wake-up path shown in the figure, which
connects five threads.  The browser main thread was signaled by a browser
worker thread as shown in step \textcircled{1} of backward slicing in Figure
\ref{fig:chromium-trace}, which in turn \vv{read\_file} in step \textcircled{2}
for IPC from a worker thread of \vv{renderer}, the daemon for rendering screens.
The \vv{renderer} worker thread is woken up by the \vv{renderer} main thread to
\vv{read\_file} \textcircled{3}, which in turn \vv{recv\_msg} \textcircled{4}
from \vv{fontd}, the font service daemon.  From this path, we could guess that
\vv{GetFirstRectForRange} was for the browser to understand the bounding box of
the search string.  \xxx further compared the wake-up path with the spinning
case, and returned the \vv{wait\_semaphore} event in the \vv{renderer} main
thread, the culprit that delayed waking up the browser main thread over 4
seconds.

What caused the wait in the \vv{renderer} main thread though?  She thus
continued diagnosis and recursively applied \xxx to the wait in \vv{renderer},
and got the wake-up path shown in the figure for this wait.  Inspection reveal
that the \vv{renderer} requested the browser's help to render Javascript and was
waiting for a reply.  At this point, a circular wait formed because the browser
was waiting for the \vv{renderer} to return the string bounding box and the
\vv{renderer} was waiting for the browser to help render Javascript.  This
circular wait was broken by a timeout in the browser main thread (the
\vv{cv\_timed\_wait} timeout was 1,500 ms).  While the system was able to make
progress, the next key press caused the spinning cursor to display for another
1,500 ms.  The timeout essentially converted a deadlock into a livelock.

\subsection{Limitations}

\xxx is designed to support interactive debugging of performance issues.  To
incrementally obtain more fine-grained event traces, it needs to rerun an
application to reproduce a performance issue.  Thus, if the issue is difficult
to reproduce, we have to rely on the log collected by the lightweight
system-wide tracing for debugging, and lose the benefits of interactivity.
Fortunately, a performance issue that almost never reproduces is probably not
as annoying as one that occurs frequently.

We implemented \xxx in the closed-source MacOS which presents a harsh test for
\xxx, but we have not ported \xxx to other operating systems yet.  It is
possible that the ideas and techniques do not generalize to other operating
systems.  However, modern operating systems share many similarities, and good
ideas tend to flow both ways, so we are hopeful that the ideas in \xxx are
generally applicable.  Similarly, the applications and performance issues used
in our evaluation may be non-representative.
