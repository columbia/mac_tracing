\section{Argus Graph Computing}\label{sec:graphcomputing}

\subsection{Event Graph}\label{subsec:eventgraph}

\xxx constructs dependency graphs with the events from tracing logs. Tracing
logs contain sequence of events per thread. Each event stands for an execution
step in a thread. They are grouped into nodes and IPCs, asynchronouns calls and
thread wakeups serve as edges; some edges can be inside a single thread.

The events traced in \xxx are carefully selected for three main purposes:
preserve semantics for the node, indentify node boundaries inside a thread,
and provide connections between nodes. We classify them into three categories:
semantics events, boundary events and connection events, as listed in
Table~\ref{table:event_types}.

\begin{table}[ht]
\begin{threeparttable}
  \centering
  \begin{tabularx}{\columnwidth}{|X|X|}
  	\hline
    \textbf{Event Type} & \textbf{Event Categories}\\
	\hline
	\hline
		System\_call & Semantics\\ \hline
		Back\_trace & Semantics\\ \hline
		NSApp\_event\tnote{1} & Semantics \\ \hline
		Wait & Semantics, Boundary \\ \hline
		Interrupts & Boundary \\ \hline
		Sharetime\_maintenance\tnote{2} & Boundary \\ \hline
		Dispatch\_invoke\tnote{3} & Boundary \\ \hline
		Runloop\_invoke & Boundary \\ \hline
		Mach\_message & Boundary, Connection\\ \hline
		Wake\_up & Connection \\ \hline
		Timer & Connection \\ \hline
		Dispatch\_enqueue & Connection \\ \hline
		Runloop\_submit & Connection \\ \hline
		Share\_flag\_read & Connection \\ \hline
		Share\_flag\_write & Connection \\ \hline
  \end{tabularx}

	\begin{tablenotes}
		\footnotesize
		\item [1] User input events dispatched to the Application.
		\item [2] Kernel invoked a routine to update timeshare quota. 
		\item [3] Invoke callback function for works from dispatch queue.
	\end{tablenotes}
 \end{threeparttable}
\caption{Event Type Categories. }
\label{table:event_types}
\end{table}

Given the prevalent of multi-threading and multi-processing programs, bugs are
much more complicated. The long opening bugs are usually have several threads
involve, even across process boundaries. As an example, the always timeout on
particular synchronization primitive in one thread usually need to trace back to
find the other thread that was responsible for signal the primitive. Compared
to the existing debugging tools like lldb and spindump, the dependency graph is
useful in that 1) it provides thread relationships all over the system across
process boundary and timing boundary and 2) it records execution history for an
input event before users capture hangs with their eyes.

\subsection{Graph Computing Algorithm}\label{subsec:graphcomputing}

In the section, we describe the algorithm \xxx uses to generate an event graph.
The algrithm has two main steps: construct nodes with heuristics based on
boundary events, and generate edges from the connection events.

A node is a sequence of events derived from the execution of a task in a thread.
As shown in Algorithm ~\ref{alg:graphcomputing}, \xxx checks events per
thread and applies heuristics when a boundary event is checked. \xxx provides
five default heuristics. Four of them share the idea of general cuasual tracing,
one is used to work around unknown programming paradigms, and more are expected
from users to improve graphs. We discuss them in below.

As listed in (\S\ref{subsec:inherentinaccuracy}), event sequences
for interrupt processing and kernel maintainance are removed from
threads in line ~\ref{graphcomputing:1.a}. The second heuristics in line
~\ref{graphcomputing:1.b} treats a wait event as an end of a node. An wait event
usually indicates a thread switches to other tasks, but it is not always true
considering a thread may park due to low thread priority. One of the examples
in MacOS is the pause of worker threads draining a low priority diapatch queue.
Dispatch queues are FIFO queues to which an application can submit tasks in
the form of block objects. Wait events should not divide the block objects,
otherwise it may result in a missing connection. To save the intergrity of block
objects, \xxx keeps a counter \vv{callout\_level} to mark if the current event
is inside a block object. Only the begin and end of Dispatch\_invoke are served
as the boundary for the node, as is shown in line~\ref{graphcomputing:1.c}.
The hueristics also applies to runloop. Runloop is an event processing loop
that used to schedule work and coordinate the receipt of incoming events.
The begin and end of the work invocation are served as boundaries. The last
default heuristics in line~\ref{graphcomputing:1.e} is used to work around the
difficulty of exploiting batch processing programming paradigms completely, as
listed in follows (\S\ref{subsec:inherentinaccuracy}). \xxx makes use of mach
messages to avoid clustering multiple tasks due to unknown batch processing. It
defines \vv{IPC\_peer\_set} to compute the set of mach message receivers/senders
for every node. For every mach message, the algorithm checks whether its peer
process exists in \vv{IPC\_peer\_set}, and adds the message into the current
node if the condition is true or the set is empty. Otherwise it adds an
ending boundary for current node and constructs a new node beginning with the
mach\_message event.

Edges connect nodes, both intra-thread and inter-thread, with connection
events. \xxx walks through event type in connection category to apply heuritics as
follows. First, the return from a wait operation causally depends on the wake-up
operation. An edge is defined from the wake-up event to the first event after
the wait returns. \xxx also add a weak edges from the wait event to the wake-up
event in line ~\ref{graphcomputing:2.a}. Mach message is the core of ipc
mechanism implemented in kernel, upon which higher level RPC are built. \xxx
connects the sender and receiver of a mach message. For messages that expect a
reply, \xxx also connects the receiver of the original message and the sender
of the reply message in line ~\ref{graphcomputing:2.b}. As discussed above,
dispatch queue and runloop are popular batch processing programming paradigms
in MacOS, \xxx connects submissions of a task and executions of the task, which
are listed in line ~\ref{graphcomputing:2.c} and line~\ref{graphcomputing:2.d}
respectively. Similarly, \xxx adds edge from a timer armed event to its
triggered event in line ~\ref{graphcomputing:2.e}. Shared flag can either be
traced with binary instrument, such as \vv{need\_display} flag for CoreAnimation
in (\S\ref{subsec:inherentinaccuracy}), or with breakpoint watcher command line
tool provided by \xxx. Edges from the flag written to its read are added from
line ~\ref{graphcomputing:2.f}. Since share variables are hard to completely
exploit, and the causality can be more complicated than the writer-reader
pattern, user interaction is expected.

Finally, a event graph returns and is subject to the improvement with
more user input heuristics.

\begin{algorithm}[ht!]
\caption{\xxx Compute Graph algorithm.}
\label{alg:graphcomputing}
\begin{algorithmic}[1]
\Require{Heuristics set + parsed tracing events}
\Ensure{Control flow graph}
\Statex
\Function{ComputeGraph}{}
\State {$callout\_level$ $\gets$ $0$}
\State {$IPC\_peer\_set$ $\gets$ $\{\}$}
\State {$current\_node$ $\gets$ $New Node$}
\For{Event: Events in a thread}
    \Switch {EventType}
	\Case {Interrupt \Or Timeshare\_maintenance} \label{graphcomputing:1.a}
		\State {$Remove following event sequence before return$}
	\EndCase
	\Case {Wait} \label{graphcomputing:1.b}
		\If {$callout\_level$ $==$ {$0$}} 
			\State {$current\_node$.$add(end\_boundary)$}
		\EndIf
	\EndCase
	\Case {Dispatch\_invoke \Or Runloop\_invoke} \label{graphcomputing:1.c}
	 	\State {$Divide each callout into a node$}
	\EndCase
	\Case {Mach\_message}\label{graphcomputing:1.d}
		\If {$IPC\_peer\_set$ $\neq$ $\emptyset$ \And $peer$ $\notin$ $IPC\_peer\_set$} 
			\State {$current\_node$.$add(end\_boundary)$}
		\EndIf
		\State {$update$ $IPC\_peer\_set$}
	\EndCase
	\EndSwitch
	\State {Other Heuristics}\label{graphcomputing:1.e}
\EndFor
\For{Event: Connection Events}
	\Switch {EventType}
	\Case {Wake\_up}\label{graphcomputing:2.a}
		\State{$AddWeakEdge(corresponding wait, wake\_up)$}
		\State{$AddEdge(wake\_up, first\_event\_after\_wait\_returns)$}
	\EndCase
	\Case {Mach\_message}\label{graphcomputing:2.b}
		\State{$AddEdge(Mach\_msg(send), Mach\_msg(receive))$}
		\If {needs reply}
			\State {$AddEdge(Mach\_msg(receive), Mach\_msg(reply\_sender))$}
		\EndIf
	\EndCase
	\Case {Dispatch\_enqueue}\label{graphcomputing:2.c}
		\State{$AddEdge(Dispatch\_enqueue, Dispatch\_invoke)$}
	\EndCase
	\Case {Runloop\_submit}\label{graphcomputing:2.d}
		\State {$AddEdge(Runloop\_submit, RunLoop\_invoke)$}
	\EndCase
	\Case {Timer}\label{graphcomputing:2.e}
		\State {$AddEdge(Timer\_armed, Timer\_callout)$}
	\EndCase
	\Case {Share\_flag\_read}\label{graphcomputing:2.f}
		\State {$AddEdge(Share\_flag\_write, Share\_flag\_read)$}
	\EndCase
	\EndSwitch
	\State {Other Heuristics}
\EndFor
\EndFunction
\end{algorithmic}
\end{algorithm}

%%\begin{figure}[tb]
%%\footnotesize\begin{verbatim}
%%Algorithm ComputeControlFlowGraph:
%%    Input: Heuristics set + parsed tracing events
%%    Output: Control flow graph
%%1. Create nodes per thread:
%%  set callout_level = 0
%%  set IPC_peer_set = 0
%%  1.a interrupt/kern_maintainance: remove_events
%%  1.b wait: end of node if callout_level == 0
%%  1.c if dispatch_callout begin, create a new node
%%      and increase callout_level
%%      if dispatch_callout end, complete current node
%%      and decrease callout_level
%%  1.d Runloop: divide every callout
%%  1.e mach_msg: divide IPC with different peers
%%  1.f Other heuristics added by user from the input set.
%%
%%2. Add edges for connection events:
%%  2.a wake_up event and wait event
%%  	edge(wake_up, first_waken_event)
%%	weak_edge(wait, wake_up)
%%  2.b mach message
%%  	edge(sender, receiver)
%%    if message needs reply
%%       edge(receiver, reply_sender)
%%  2.c dispatch queue events
%%  	edge(work_enqueue, work_dequeue)
%%  2.d Runloop work submission
%%  	edge(work_submit, work_invoke)
%%  2.e timer events
%%  	edge(timer_armed, timer_fired)
%%  2.f shared variable
%%  	edge(set_variable, read_and_clear_variable)
%%  2.g Other heuristics added by user from the input set.
%%3. Return the Graph with Nodes and Edges
%%\end{verbatim}
%%    \caption{\xxx Compute Graph algorithm.}
%%    \label{fig:alg-graphcomputing}
%%\end{figure}

\subsection{Inherent Inaccuracy}\label{subsec:inherentinaccuracy}

Constructing an accurate and complete dependency graph is difficult, if not
formally undecidable. The graph is inherently inaccurate. That one thread
wakes up the other thread does not always imply a causility between them. In
implementation, \xxx handles the following types of spurious edges.

\begin{itemize}

\item interrupt processing and kernel sharetime maintanance that take over
current thread context.

\item timer expiration in the kernel which clears up all the waiting threads on
an event source.

\end{itemize}

\noindent In addition to the known definitive noise, there exist false connections and
missing dependencies based on our experience of building a dependency graph
with traditional causality tracing. We define them as over connection and under
connection respectively.

\vspace{1.5mm}
\subsubsection{Over Connections}\hfill\\
\vspace{-0.5mm}
Over connections usually occur when intra-thread boundaries are missing due to
unknown batch processing programming paradigms. We list the example patterns we
found below.

\paragraph{Dispatch message batching}

While traditional causual tracing assumes the entire execution of a callback
function is on behalf of one request, we found some daemons implement their
service loop inside the callback function and create false dependencies. In the
code snippet below from the \vv{fontd} daemon, function \vv{dispatch\_execute}
is installed as a callback to a work from dispatch queue. It subsequently calls
\vv{dispatch\_mig\_server()} which runs the typical server loop and handles
messages from different apps.

To avoid incorrectly linking many irrelevant processes through such batching
processing patterns, \xxx adopts the aforementioned heuristics to split an
execution segment when it observes that the segment sends out messages to two
distinct processes. Any capplication or daemon can implement its own server loop
this way, which makes it fundamentally difficult to automatically infer event
handling boundaries.

\vspace{-4mm}
\begin{figure}[ht!]
\begin{minipage}[t]{.25\textwidth}
\begin{lstlisting}
//worker thread in fontd:
//enqueue a block
block = dispatch_mig_sevice;
dispatch_async(block);
\end{lstlisting}
\end{minipage}\hfill
\begin{minipage}[t]{.21\textwidth}
\begin{lstlisting}
//main thread in fontd:
//dequeue blocks
block = dequeue();
dispatch_execute(block);
\end{lstlisting}
\end{minipage}

\begin{minipage}[t]{0.48\textwidth}
\begin{lstlisting}
//implementation of dipatch_mig_server
dispatch_mig_server()
for(;;) //batch processing
  mach_msg(send_reply, recv_request)
  call_back(recv_request)
  set_reply(send_reply)
\end{lstlisting}
\end{minipage}
   %% \caption{Dispatch message batching}
    \label{fig:dispatchmessagebatching}
\end{figure}
\vspace{-7mm}

\paragraph{Batching in event processing}

Message activities inside a system call are assumed to be related traditionally.
However, to presumably save on kernel boundary crossings, WindowServer MacOS
system daemon uses a single system call to receive data and send data for an
unrelated event from differnt processed in its event loop. This batch processing
artificially makes many events appear dependent. We split the execution segments
to maintain the independence of the events.

\vspace{-4mm}
\begin{figure}[ht!]
\begin{lstlisting}
//inside a single thread
while() {
  CGXPostReplyMessage(msg) {
  // send _gOutMsg if it hasn't been sent
    push_out_message(_gOutMsg)
    _gOutMsg = msg
    _gOutMessagePending = 1
  }
  CGXRunOneServicePass() {
    if (_gOutMessagePending)
      mach_msg_overwrite(MSG_SEND | MSG_RECV, _gOutMsg)
    else
      mach_msg(MSG_RECV)
    ... // process received message
  }
}
\end{lstlisting}
    %%\caption{Batching in event processing}
    \label{fig:batchingineventprocessing}
\end{figure}
\vspace{-1.5mm}

\subsubsection{Under Connections}\hfill\\
On the other hand, under connections mostly result from missing data
dependencies. Data dependencies, both inter- and intra-thread, are usually hard
to fully exploit in the initial pass of graph computing.

\paragraph{Data dependency in event processing}
The code for \textbf{Batching in event processing} above also illustrates a causal
linkage caused by data dependency in one thread. WindowServer saves the reply
message in variable \vv{\_gOutMsg} inside function \vv{CGXPostReplyMessage}.
When it calls \vv{CGXRunOneServicePass}, it sends out \vv{\_gOutMsg} if there
is any pending message. \xxx uses watch point registers to capture events
on \vv{\_gOutMsg} and establish the causal link between the handling of the
previous request and the send of the reply.

\paragraph{CoreAnimation shared flags}
%As shown in Figure~\ref{fig:casharedflag}, worker thread can set
As shown in the code snippet below, worker thread can set
a field \vv{need\_display} inside a CoreAnimation
object whenever the object needs to be repainted. The main thread iterates over
all animation objects and reads this flag, rendering any such object. This
shared-memory communication creates a dependency between the main thread and the
worker so accesses to these field flags need to be tracked.
%%However, since each object has such a field flag, \xxx cannot afford to monitor
%%each using a watch point register. Instead, it uses instrumentation to modify
%%the CoreAnimation library to trace events on these flags.
\vspace{-4mm}
\begin{figure}[ht!]
\begin{minipage}[t]{.20\textwidth}
\begin{lstlisting}
//Worker thread:
//needs to update UI:
obj->need_display = 1
\end{lstlisting}\hfill
\end{minipage}
\noindent\begin{minipage}[t]{.28\textwidth}
\begin{lstlisting}
//Main thread: 
//traverse all CA objects
if(obj->need_display == 1)
  render(obj)
\end{lstlisting}\hfill
\end{minipage}
    %%\caption{CoreAnimation shared flag}
    \label{fig:casharedflag}
\end{figure}
\vspace{-10mm}

\paragraph{Spinning cursor shared flag}
As shown in Figure~\ref{fig:spinningcursorsharedflags},
whenever the system determines that the main thread has hung for a certain
period, and the spinning beach ball should be displayed, a shared-memory flag
is set. Access to the flag is controlled via a lock, i.e. the lock is used for
mutual exclusion, and does not imply a happens before relationship. Thus, \xxx
captures accesses to these flags using watch-point registers to add causal edges
correctly.
\begin{figure*}[ht!]
\begin{minipage}[t]{0.5\textwidth}
\begin{lstlisting}
//NSEvent thread:
CGEventCreateNextEvent() {
  if (sCGEventIsMainThreadSpinning == 0x0)
     if (sCGEventIsDispatchToMainThread == 0x1)
       CFRunLoopTimerCreateWithHandler{
         if (sCGEventIsDispatchToMainThread == 0x1)
           sCGEventIsMainThreadSpinning = 0x1
           CGSConnectionSetSpinning(0x1);
       }
}
\end{lstlisting}
\end{minipage}
\begin{minipage}[t]{0.4\textwidth}
\begin{lstlisting}
//Main thread:
{
	... //pull events from event queue
	Convert1CGEvent(0x1);
	if (sCGEventIsMainThreadSpinning == 0x1){
  		CGSConnectionSetSpinning(0x0);
  		sCGEventIsMainThreadSpinning = 0x0;
  		sCGEventIsDispatchedToMainThread = 0x0;
	}
}
\end{lstlisting}
\end{minipage}
    \caption{Spinning Cursor Shared Flags}
    \label{fig:spinningcursorsharedflags}
\end{figure*}

\subsection{User Interactions}\label{subsec:userinteraction}
%%Main point for the subsection: why we need user interaction, what the user can
%%do and how it helps
%% Para1 : why we need user interaction
%% Para2 : what the user can do
%% Para3 : how it helps causality tracing
In addition to batching processing and data dependency, the spurious edge
introduced by mutex lock is also a challenge in debugging. The synchronization
on mutex lock reflects one thread wakes up the other thread. The scenario can
either be a producer-consumer problem or merely mutual exclusion. Making the
graph completely sound without user interaction is almost impossible given the
essential attribute of commercial operating system as a grey box.

As we mentioned above, to figure out all over connections and under connections
before hand is almost impossible. Instead, users can find out the discrepancy
in event graph after the initial graph computing. For example, we noticed that
two unrelated applications connects to one node in the graph, which leads to the
manifestation of kernel thread batch processing on timers. Like other causality
tracing approaches, \xxx is a general framework for macOS and tested on limited
programming paradigms. Allowing user interaction makes its graph more practical
and useful case by case.

Users can gradually inject their knowledge to improve the graph in two ways.
They can either add heuristics to the graph computing, or use binary instrument
with the APIs provided by \xxx to expand the boundary or connection event
categories.
