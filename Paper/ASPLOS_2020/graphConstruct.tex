\section{Argus Graph Computing}\label{sec:graphcomputing}

\subsection{Event Graph}\label{subsec:eventgraph}

\xxx constructs dependency graphs with the events from tracing logs. Tracing
logs contain sequence of events per thread. Each event stands for an execution
step in a thread. They are grouped into nodes and IPCs, asynchronouns calls and
thread wakeups serve as edges; some edges can be inside a single thread.

The events traced in \xxx are carefully selected for three main purposes:
preserve semantics for the node, indentify node boundaries inside a thread,
and provide connections between nodes. We classify them into three categories:
semantics events, boundary events and connection events, as listed in
Table~\ref{table:event_types}.

\begin{table}[ht]
\begin{threeparttable}
  \centering
  \begin{tabularx}{\columnwidth}{|X|X|}
  	\hline
    \textbf{Event Type} & \textbf{Event Categories}\\
	\hline
	\hline
		System\_call & Semantics\\ \hline
		Back\_trace & Semantics\\ \hline
		NSApp\_event\tnote{1} & Semantics \\ \hline
		Wait & Semantics, Boundary \\ \hline
		Interrupts & Boundary \\ \hline
		Sharetime\_maintenance\tnote{2} & Boundary \\ \hline
		Dispatch\_invoke\tnote{3} & Boundary \\ \hline
		Runloop\_invoke & Boundary \\ \hline
		Mach\_message & Boundary, Connection\\ \hline
		Wake\_up & Connection \\ \hline
		Timer & Connection \\ \hline
		Dispatch\_enqueue & Connection \\ \hline
		Runloop\_submit & Connection \\ \hline
		Share\_flag\_read & Connection \\ \hline
		Share\_flag\_write & Connection \\ \hline
  \end{tabularx}

	\begin{tablenotes}
		\footnotesize
		\item [1] User input events dispatched to the Application.
		\item [2] Kernel invoked a routine to update timeshare quota. 
		\item [3] Invoke callback function for works from dispatch queue.
	\end{tablenotes}
 \end{threeparttable}
\caption{Event Type Categories. }
\label{table:event_types}
\end{table}

Given the prevalent of multi-threading and multi-processing programs, bugs are
much more complicated. The long opening bugs are usually have several threads
involve, even across process boundaries. As an example, the always timeout on
particular synchronization primitive in one thread usually need to trace back to
find the other thread that was responsible for signal the primitive. Compared
to the existing debugging tools like lldb and spindump, the dependency graph is
useful in that 1) it provides thread relationships all over the system across
process boundary and timing boundary and 2) it records execution history for an
input event before users capture hangs with their eyes.

\subsection{Graph Computing Algorithm}\label{subsec:graphcomputing}

In the section, we describe the algorithm \xxx uses to generate an event graph.
The algrithm has two main steps: construct nodes with heuristics based on
boundary events, and generate edges from the connection events.

A node is a sequence of events derived from the execution of a task in a thread.
As shown in Algorithm ~\ref{alg:graphcomputing}, \xxx checks events per
thread and applies heuristics when a boundary event is checked. \xxx provides
five default heuristics. Four of them share the idea of general cuasual tracing,
one is used to work around unknown programming paradigms, and more are expected
from users to improve graphs. We discuss them in below.

As listed in (\S\ref{sec:inherentinaccuracy}), event sequences
for interrupt processing and kernel maintainance are removed from
threads in line ~\ref{graphcomputing:1.a}. The second heuristics in line
~\ref{graphcomputing:1.b} treats a wait event as an end of a node. An wait event
usually indicates a thread switches to other tasks, but it is not always true
considering a thread may park due to low thread priority. One of the examples
in MacOS is the pause of worker threads draining a low priority diapatch queue.
Dispatch queues are FIFO queues to which an application can submit tasks in
the form of block objects. Wait events should not divide the block objects,
otherwise it may result in a missing connection. To save the intergrity of block
objects, \xxx keeps a counter \vv{callout\_level} to mark if the current event
is inside a block object. Only the begin and end of Dispatch\_invoke are served
as the boundary for the node, as is shown in line~\ref{graphcomputing:1.c}.
The hueristics also applies to runloop. Runloop is an event processing loop
that used to schedule work and coordinate the receipt of incoming events.
The begin and end of the work invocation are served as boundaries. The last
default heuristics in line~\ref{graphcomputing:1.e} is used to work around the
difficulty of exploiting batch processing programming paradigms completely, as
listed in follows (\S\ref{sec:inherentinaccuracy}). \xxx makes use of mach
messages to avoid clustering multiple tasks due to unknown batch processing. It
defines \vv{IPC\_peer\_set} to compute the set of mach message receivers/senders
for every node. For every mach message, the algorithm checks whether its peer
process exists in \vv{IPC\_peer\_set}, and adds the message into the current
node if the condition is true or the set is empty. Otherwise it adds an
ending boundary for current node and constructs a new node beginning with the
mach\_message event.

Edges connect nodes, both intra-thread and inter-thread, with connection
events. \xxx walks through event type in connection category to apply heuritics as
follows. First, the return from a wait operation causally depends on the wake-up
operation. An edge is defined from the wake-up event to the first event after
the wait returns. \xxx also add a weak edges from the wait event to the wake-up
event in line ~\ref{graphcomputing:2.a}. Mach message is the core of ipc
mechanism implemented in kernel, upon which higher level RPC are built. \xxx
connects the sender and receiver of a mach message. For messages that expect a
reply, \xxx also connects the receiver of the original message and the sender
of the reply message in line ~\ref{graphcomputing:2.b}. As discussed above,
dispatch queue and runloop are popular batch processing programming paradigms
in MacOS, \xxx connects submissions of a task and executions of the task, which
are listed in line ~\ref{graphcomputing:2.c} and line~\ref{graphcomputing:2.d}
respectively. Similarly, \xxx adds edge from a timer armed event to its
triggered event in line ~\ref{graphcomputing:2.e}. Shared flag can either be
traced with binary instrument, such as \vv{need\_display} flag for CoreAnimation
in (\S\ref{sec:inherentinaccuracy}), or with breakpoint watcher command line
tool provided by \xxx. Edges from the flag written to its read are added from
line ~\ref{graphcomputing:2.f}. Since share variables are hard to completely
exploit, and the causality can be more complicated than the writer-reader
pattern, user interaction is expected.

Finally, a event graph returns and is subject to the improvement with
more user input heuristics.

\begin{algorithm}[ht!]
\caption{\xxx Compute Graph algorithm.}
\label{alg:graphcomputing}
\begin{algorithmic}[1]
\Require{Heuristics set + parsed tracing events}
\Ensure{Control flow graph}
\Statex
\Function{ComputeGraph}{}
\State {$callout\_level$ $\gets$ $0$}
\State {$IPC\_peer\_set$ $\gets$ $\{\}$}
\State {$current\_node$ $\gets$ $New Node$}
\For{Event: Events in a thread}
    \Switch {EventType}
	\Case {Interrupt \Or Timeshare\_maintenance} \label{graphcomputing:1.a}
		\State {$Isolate\_event\_sequence\_before\_return\_into\_a\_new\_node$}
	\EndCase
	\Case {Wait} \label{graphcomputing:1.b}
		\If {$callout\_level$ $==$ {$0$}} 
			\State {$current\_node$.$add(end\_boundary)$}
		\EndIf
	\EndCase
	\Case {Dispatch\_invoke \Or Runloop\_invoke} \label{graphcomputing:1.c}
	 	\State {$Divide\_each\_callout\_into\_a_node$}
	\EndCase
	\Case {Mach\_message}\label{graphcomputing:1.d}
		\If {$IPC\_peer\_set$ $\neq$ $\emptyset$ \And $peer$ $\notin$ $IPC\_peer\_set$} 
			\State {$current\_node$.$add(end\_boundary)$}
		\EndIf
		\State {$update$ $IPC\_peer\_set$}
	\EndCase
	\EndSwitch
	\State {Other Heuristics}\label{graphcomputing:1.e}
\EndFor
\For{Event: Connection Events}
	\Switch {EventType}
	\Case {Wake\_up}\label{graphcomputing:2.a}
		\State{$AddWeakEdge(wait, wake\_up)$}
		\State{$AddEdge(wake\_up, first\_event\_after\_wait\_returns)$}
	\EndCase
	\Case {Mach\_message}\label{graphcomputing:2.b}
		\State{$AddEdge(msg(send), msg(receive))$}
		\If {needs reply}
			\State {$AddEdge(msg(receive), msg(reply\_send))$}
		\EndIf
	\EndCase
	\Case {Dispatch\_enqueue}\label{graphcomputing:2.c}
		\State{$AddEdge(Dispatch\_enqueue, Dispatch\_invoke)$}
	\EndCase
	\Case {Runloop\_submit}\label{graphcomputing:2.d}
		\State {$AddEdge(Runloop\_submit, RunLoop\_invoke)$}
	\EndCase
	\Case {Timer}\label{graphcomputing:2.e}
		\State {$AddEdge(Timer\_armed, Timer\_callout)$}
	\EndCase
	\Case {Share\_flag\_read}\label{graphcomputing:2.f}
		\State {$AddEdge(Share\_flag\_write, Share\_flag\_read)$}
	\EndCase
	\EndSwitch
	\State {Other Heuristics}
\EndFor
\EndFunction
\end{algorithmic}
\end{algorithm}

%%\begin{figure}[tb]
%%\footnotesize\begin{verbatim}
%%Algorithm ComputeControlFlowGraph:
%%    Input: Heuristics set + parsed tracing events
%%    Output: Control flow graph
%%1. Create nodes per thread:
%%  set callout_level = 0
%%  set IPC_peer_set = 0
%%  1.a interrupt/kern_maintainance: remove_events
%%  1.b wait: end of node if callout_level == 0
%%  1.c if dispatch_callout begin, create a new node
%%      and increase callout_level
%%      if dispatch_callout end, complete current node
%%      and decrease callout_level
%%  1.d Runloop: divide every callout
%%  1.e mach_msg: divide IPC with different peers
%%  1.f Other heuristics added by user from the input set.
%%
%%2. Add edges for connection events:
%%  2.a wake_up event and wait event
%%  	edge(wake_up, first_waken_event)
%%	weak_edge(wait, wake_up)
%%  2.b mach message
%%  	edge(sender, receiver)
%%    if message needs reply
%%       edge(receiver, reply_sender)
%%  2.c dispatch queue events
%%  	edge(work_enqueue, work_dequeue)
%%  2.d Runloop work submission
%%  	edge(work_submit, work_invoke)
%%  2.e timer events
%%  	edge(timer_armed, timer_fired)
%%  2.f shared variable
%%  	edge(set_variable, read_and_clear_variable)
%%  2.g Other heuristics added by user from the input set.
%%3. Return the Graph with Nodes and Edges
%%\end{verbatim}
%%    \caption{\xxx Compute Graph algorithm.}
%%    \label{fig:alg-graphcomputing}
%%\end{figure}


\subsection{User Interactions}\label{subsec:userinteraction}
%%Main point for the subsection: why we need user interaction, what the user can
%%do and how it helps
%% Para1 : why we need user interaction
%% Para2 : what the user can do
%% Para3 : how it helps causality tracing
In addition to batching processing and data dependency, the spurious edge
introduced by mutex lock is also a challenge in debugging. The synchronization
on mutex lock reflects one thread wakes up the other thread. The scenario can
either be a producer-consumer problem or merely mutual exclusion. Making the
graph completely sound without user interaction is almost impossible given the
essential attribute of commercial operating system as a grey box.

As we mentioned above, to figure out all over connections and under connections
before hand is almost impossible. Instead, users can find out the discrepancy
in event graph after the initial graph computing. For example, we noticed that
two unrelated applications connects to one node in the graph, which leads to the
manifestation of kernel thread batch processing on timers. Like other causality
tracing approaches, \xxx is a general framework for macOS and tested on limited
programming paradigms. Allowing user interaction makes its graph more practical
and useful case by case.

Users can gradually inject their knowledge to improve the graph in two ways.
They can either add heuristics to the graph computing, or use binary instrument
with the APIs provided by \xxx to expand the boundary or connection event
categories.
