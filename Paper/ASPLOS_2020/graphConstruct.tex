\section{Argus Graph Computing}\label{sec:graphcomputing}

\subsection{Event Graph}\label{subsec:eventgraph}
%%Main point for the subsection: \xxx computes a dependency graph to assist
%%debugging. It is a useful graph to figure out relationships across thread
%%boundary and timing boundary 
%%
%%\begin{itemize}
%%\item What the graph is?
%%
%%	\begin{itemize}
%%	\item Paragraph1: high level description of dependency graph.
%%
%%\xxx constructs dependency graphs with the events from tracing logs. Tracing
%%logs contains sequence of events per thread. Each event stands for an execution
%%step in the thread. They are grouped into nodes and IPCs, asynchronouns calls
%%and thread wakeups serve as edges; some edges can be inside a single thread.
%%
%%	\item Paragraph2: detail about the graph.
%%
%%Events can be classed into three categories: semantic, connection, boundary
%%Graph nodes consist of a list of execution events and edges are generated with
%%event pairs.
%%
%%	\end{itemize}
%%
%%\item Why the graph is userful?
%%The graph bares the causility path of a user input and thus is helpful in
%%debugging complicated performance bugs, which involve mutilple processes and
%%threads.
%%\end{itemize}

\xxx constructs dependency graphs with the events from tracing logs. Tracing
logs contains sequence of events per thread. Each event stands for an execution
step in the thread. They are grouped into nodes and IPCs, asynchronouns calls
and thread wakeups serve as edges; some edges can be inside a single thread.

The event types traced in \xxx are carefully selected for three main
purpose: preserve semantics for the node, indentify node boundary inside
a thread, and provide connections between nodes. As is listed in Table
\ref{Table_event_types}.

Given the prevalent of multi-threading and multi-processing programs, bugs are
much more complicated. The long opening bugs are usually have several threads
involve, even across process boundaries. As an example, the always timeout on
particular synchronization primitive in one thread should be traced back to the
other thread that was responsible for signal on the primitive. Compared to the
existing debugging tools like lldb and spindump, the dependency graph is useful
in that 1) it provides thread relationships all over the system across processes
boundary and timing boundary and 2) it records execution history for an input
event before users capture hangs with their eyes.

\subsection{Inherent Inaccuracy}
%%main point: the inaccuracy of causality graph
%%\begin{itemize}
%% 	\item Paragraph1: describe the inaccuracy with general idea, define over
%%connection and under connection
%%
%%	
%%	\item Paragraph2: over connection patterns
%%
%%Over connections occur if intra-thread boundaries are missing from batch
%%processing programming paradigms. (dispatch\_mig\_service, runloop)
%%
%%	\item Paragraph3: under connection patterns
%%
%%Data dependencies inter/intra threads are usually hard to fully exploit in the
%%initial pass of graph computing. (shared flags in Object, data dependency for
%%delay work intra-thread)
%%
%%\end{itemize}

However, to construct an accurate and complete dependency graph is difficult,
if not impossible. The graph is inherently inaccurate. That one thread wakes
up the other thread does not always stand for a causility between them. In
implementation, \xxx filter out some of the definitive noise in the following
types. We identify the purpose of wake-up and add huristics to filter out the
false edges.

\begin{itemize}

\item interrupt processing and kernel sharetime maintanance that take over
current thread context.

\item timer expiration in the kernel which clears up all the waiting threads on
an event source.

\end{itemize}

In addition to the known definitive noise, there exist false connections and
miss data dependencies based on our experience while building dependency graph
with traditional causality tracing. We define them as over connection and under
connection respectively.

Over connections occur if intra-thread boundaries are missing from batch
processing programming paradigms. (dispatch\_mig\_service, runloop)

\para{Dispatch message batching}
The message dispatch service dequeues messages from many processes and staggers
processing of the messages. This creates false dependencies between each
message in the dispatch queue.  As illustrated in the following code snipped
from the \vv{fontd} daemon, function \vv{dispatch\_execute} is installed as a
callback to a dispatch queue.  It subsequently calls
\vv{dispatch\_mig\_server()} which runs the typical server loop and handles many
messages.
To avoid incorrectly linking many irrelevant processes through such
batching processing patterns, \xxx adopts the aforementioned heuristics to
split an execution segment when it observes that the segment sends out messages
to two distinct processes.
This pattern does pose a challenge for automated causal tracing tools that
assume that the entire execution of a callback function is on behalf of one
request.  The code shown uses a dispatch-queue callback, but inside the
callback, it does work on behalf of many different requests.  Any application
or daemon can implement its  own server loop this way, which makes it
fundamentally difficult to automatically infer event handling boundaries.

{\footnotesize \begin{verbatim}
Worker thread in fontd daemon:
dispatch_async(block)

Main thread in fontd daemon:
block = dispatch_queue.dequeue()
dispatch_execute(block)
  dispatch_mig_server()

dispatch_msg_server()
  for(;;)
    mach_msg(send_reply, recev_request)
    call_back()
    set_reply()
\end{verbatim}
}

\para{Runloop callbacks batch processing}
As is common in event driven programming, many methods can post a callback and
MacOS uses runloop as a common idiom to process callbacks.  As shown in the
following step-by-step description of the MacOS runloop, an iteration of the
runloop does 10 different stages of processing, each of which may do work on
behalf of completely irrelevant requests.  Since there are no obvious events
(\eg, a wait operation) to split the execution, \xxx uses instrumentation to
add beginning and ending points for MacOS runloops.  In general, any
application or daemon can create its own version of the runloop, posing
challenges for automated inference of event processing boundaries.
% // another thread installs cb
% performSelector:onThread:withObject:waitUntilDone;

{\footnotesize \begin{verbatim}
Run loop sequence of events //developer.apple.com
1-3.Notify observers
4.Fire any non-port-based input sources
5.If a port-based input source is ready and waiting to fire,
    process the event immediately. Go to step 9.
6.Notify observers that the thread is about to sleep.
7.Put the thread to sleep until:
    //one of the following events occurs
    An event arrives for a port-based input source.
    A timer fires.
    The timeout value set for the run loop expires.
    The run loop is explicitly woken up.
8.Notify observers that the thread just woke up.
9.Process the pending event.
  If a user-defined timer fired,
    process the timer event
    restart the loop.
    Go to step 2.
  If an input source fired
    deliver the event.
  If the run loop was explicitly woken up, but not timed out,
    restart the loop. Go to step 2.
10.Notify observers that the run loop has exited.

\end{verbatim}
}

\para{Batching and data dependency in event processing}
The WindowServer MacOS system daemon contains an event loop which waits on Mach
messages. Conceptually, it processes a series of independent events from
different processes. However, to presumably save on kernel boundary crossings,
it uses a single system call to receive data and send data for an unrelated
event. This batch processing artificially makes many events appear dependent,
and we split the execution segments to maintain the independence of the events.

This case also illustrates a causal linkage caused by data dependency within
one thread.  As the code shows, WindowServer saves the reply message in
variable \vv{\_gOutMsg} inside function \vv{CGXPostReplyMessage}.  When it calls
\vv{CGXRunOneServicePass}, it sends out \vv{\_gOutMsg} if there is any pending
message.  This data dependency needs to be captured in order to establish a
causal link between the handling of the previous request and the send of the
reply.  Interestingly, it is an example of a data dependency within the same
thread.  \xxx uses watch point registers to capture events on these data flags
and establish causal links between them.

{\footnotesize \begin{verbatim}
while() {
  CGXPostReplyMessage(msg) {
  // send _gOutMsg if it hasn't been sent
    push_out_message(_gOutMsg)
    _gOutMsg = msg
    _gOutMessagePending = 1
  }
  CGXRunOneServicePass() {
    if (_gOutMessagePending)
      mach_msg_overwrite(MSG_SEND | MSG_RECV, _gOutMsg)
    else
      mach_msg(MSG_RECV)
    ... // process received message
  }
}
\end{verbatim}
}

Under connections are result from particular programing paradigms and data
dependencies. Data dependencies inter/intra threads are usually hard to fully
exploit in the initial pass of graph computing. (shared flags in Object, data
dependency for delay work intra-thread)

\para{CoreAnimation shared flags}
A worker thread can set a field \vv{need\_display} inside a CoreAnimation object
whenever the object needs to be repainted. The main thread iterates over all
animation objects and reads this flag, rendering any such object.

This shared-memory communication creates a dependency between the main thread
and the worker so accesses to these field flags need to be tracked.  However,
since each object has such a field flag, \xxx cannot afford to monitor each
using a watch point register.  Instead, it uses instrumentation to modify the
CoreAnimation library to trace events on these flags.

{\footnotesize \begin{verbatim}
Worker thread that needs to update UI:
ObjCoreAnimation->need_display = 1

Main thread: 
traverse all CoreAnimationobjects
if (obj->need_display == 1)
  render(obj)

\end{verbatim}
}

\para{Spinning cursor shared flag}
Whenever the system determines that the main thread has hung for a certain
period, and the spinning beach ball should be displayed, a shared-memory flag
is set. Access to this flag is controlled via a lock, i.e. the lock is used for
mutual exclusion, and does not imply a happens before relationship.  Thus, \xxx
captures accesses to these flags using watch-point registers to add causal
edges correctly.

{\footnotesize \begin{verbatim}
NSEvent thread:
CGEventCreateNextEvent() {
  if (sCGEventIsMainThreadSpinning == 0x0)
     if (sCGEventIsDispatchToMainThread == 0x1)
       CFRunLoopTimerCreateWithHandler{
         if (sCGEventIsDispatchToMainThread == 0x1)
           sCGEventIsMainThreadSpinning = 0x1
           CGSConnectionSetSpinning(0x1);
       }
}

Main thread
Convert1CGEvent(0x1);
if (sCGEventIsMainThreadSpinning == 0x1)
  CGSConnectionSetSpinning(0x0);
  sCGEventIsMainThreadSpinning = 0x0;
  sCGEventIsDispatchedToMainThread = 0x0;
\end{verbatim}
}

In addition, the spurious edge introduced by mutex lock is also a challenge in
debugging. The synchronization on mutex lock reflects one thread wakes up the
other thread. They can depend on each other like producer and comsumer, while
no causality exists in the case of contention for a shared resource. Making the
graph completely sound without user interaction is almost impossible given the
essential attribite of commericial operating system as a grey box.

\subsection{User Interactions}

%%Main point for the subsection: why we need user interaction, what the user can
%%do and how it helps
%% Para1 : why we need user interaction
%% Para2 : what the user can do
%% Para3 : how it helps causality tracing

As we mentioned above, to figure out all the over connections and under
connections before hand is almost impossible. Instead, users can find out the
descrepency on the dependency graph while checking the computing result. For
example, we noticed that two unrelated applications connects to one node in the
graph, which leads to the manifestation of kernel thread batch processing on
timers. Users can gradually inject their knowledge until the graph is reasonable
for following debugging. 

Over connections in this kind can be eliminated by adding heuristics to the
process of graph computing. In addition, users can also binary instrument the
image with the APIs provided by \xxx to amending the missing boundaries. On
contrary, if users discover under connections due to missing data dependency,
they can make use of the \xxx's hardware breakpointer tool to monitor the data
and add rules to recompute the graph.

Like other causality tracing approaches, \xxx is a general framework and tested
on limited data set. Allowing user interaction makes the dependency graph more
practical and useful case by case.

\subsection{Graph Computing Algorithm}
Graph computing divides the thread into multiple nodes base on the heuristics
defined for boundary events. Interrupts and kernel maintainance will be isolated
from the current thread. To save the integrety of works drained from dispatch
queue, it keeps a counter to monitor if current event is inside a dispatch queue
item. \xxx makes use of mach message to add a heuristic to exclude a super node
that includes execution steps for muliple tasks. \xxx defines IPC peer set for
every node, which records the processes communicated with current node. For
every mach message, \xxx gets the IPC peer set of the current node. If the
set is empty, \xxx add the message event into the current node and update the
peer set with the message. Otherwise, it check the peer and the voucher of the
current message, if neither the peer or the processes in the voucher appeared in
the IPC peer set, a new node is created for current message and its following
events.

The next step is to generate edges with the hueristics defined for the
connection events. wake-up and wait are the most frequent events for the purpose
of connection, we faked a woken event after every wait event with the timestamp
when it wakes from the wait. The edges comes from the wake-up event to the woken
event. \xxx also create a weak edge from the wait event to the corresponding
wake-up event. mach messages are the lowlevel ipc mechanism in MacOS, we
connects the threads involved in IPC message. Dispatch queue and runloop are
popular programing paradigms to implement the asynchronous work, \xxx create
edges respectively. As \xxx provides hardware breakpointer to collect data
dependency, the graph computing algorithm create edges from the variable writing
event to its reading event.

Finally, the computing graph is generated and subject to the improvement with
more input heuristics.

\begin{figure}[tb]
\footnotesize\begin{verbatim}
Algorithm ComputeControlFlowGraph:
    Input: Heuristics set + parsed tracing events
    Output: Control flow graph
1. Divide event per thread into nodes:
  set callout_level = 0
  1.a interrupt/kern_maintainance: remove_events
  1.b wait: end of node if callout_level == 0
  1.c if dispatch_callout begin, create a new node
      and increase callout_level
      if dispatch_callout end, complete current node
      and decrease callout_level
  1.d Runloop: divide every callout
  1.e mach_msg: divide IPC with different peers
  1.f Other heuristics added by user from the input set.

2. For each Connection event type, generate edges:
  2.a wake-up event and wait event
    weak edge from wait event to wakeup
    edge from the wake-up event to the first waken event
  2.b mach message
    edge from the sender to the receiver
    edge from the receive to reply sender
    edge from the reply sender to reply receiver
  2.c timer events
    edge from timer create to timer callout
  2.d dispatch queue events
    edge from the dispatch enqueu event to dispatch dequeue
  2.e Create edges from Runloop work submission
    to the RunLoop work execution.
  2.f Create edges from the CoreAnimation
    needsdisplay event to the display event
  2.g Create edges from the hardware breakpoint write event
    to its read event.
  2.h Other heuristics added by user from the input set.
3. Return the Graph with Nodes and Edges
\end{verbatim}
    \caption{\xxx Compute Graph algorithm.}
    \label{fig:alg-graphcomputing}
\end{figure}
