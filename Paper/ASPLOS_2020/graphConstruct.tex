\section{Argus Graph Computing}\label{sec:graphcomputing}

\subsection{Event Graph}\label{subsec:eventgraph}
%%Main point for the subsection: \xxx computes a dependency graph to assist
%%debugging. It is a useful graph to figure out relationships across thread
%%boundary and timing boundary 
%%
%%\begin{itemize}
%%\item What the graph is?
%%
%%	\begin{itemize}
%%	\item Paragraph1: high level description of dependency graph.
%%
%%\xxx constructs dependency graphs with the events from tracing logs. Tracing
%%logs contains sequence of events per thread. Each event stands for an execution
%%step in the thread. They are grouped into nodes and IPCs, asynchronouns calls
%%and thread wakeups serve as edges; some edges can be inside a single thread.
%%
%%	\item Paragraph2: detail about the graph.
%%
%%Events can be classed into three categories: semantic, connection, boundary
%%Graph nodes consist of a list of execution events and edges are generated with
%%event pairs.
%%
%%	\end{itemize}
%%
%%\item Why the graph is userful?
%%The graph bares the causility path of a user input and thus is helpful in
%%debugging complicated performance bugs, which involve mutilple processes and
%%threads.
%%\end{itemize}

\xxx constructs dependency graphs with the events from tracing logs. Tracing
logs contain sequence of events per thread. Each event stands for an execution
step in a thread. They are grouped into nodes and IPCs, asynchronouns calls
and thread wakeups serve as edges; some edges can be inside a single thread.

The events traced in \xxx are carefully selected for three main purposes:
preserve semantics for the node, indentify node boundary inside a thread, and
provide connections between nodes. We classify them into three categories:
semantics events, boundary events and connection events, as listed in
Table~\ref{table:event_types}.

\begin{table}[h]
  \centering
  \begin{tabularx}{\columnwidth}{|X|X|}
  	\hline
    \textbf{Categories} & \textbf{Event types}\\
	\hline
	\hline
    {\bf Semantics Events} & System\_call\\
    provide hints for user & Back\_trace\\					   
	interactive debugging  & NSApp\_event\\
    \hline
    {\bf Boundary Events} & Interrupts\\
	construct nodes & Sharetime\_maintenance\\
	& Wait\\
	& Dispatch\_invoke\\
    & Runloop\_invoke\\
	& Mach\_message\\
    \hline
	{\bf Connection Events} & Wake\_up\\
    add edges & Mach\_message\\
    & Dispatch\_enqueue\\
    & Runloop\_submit\\
	& Share\_flag\_write\\
	& Share\_flag\_read\\
    \hline
  \end{tabularx}
  \caption{Event Type Categories. }
  \label{table:event_types}
\end{table}

Given the prevalent of multi-threading and multi-processing programs, bugs are
much more complicated. The long opening bugs are usually have several threads
involve, even across process boundaries. As an example, the always timeout on
particular synchronization primitive in one thread usually need to trace back to
find the other thread that was responsible for signal on the primitive. Compared
to the existing debugging tools like lldb and spindump, the dependency graph is
useful in that 1) it provides thread relationships all over the system across
processes boundary and timing boundary and 2) it records execution history for
an input event before users capture hangs with their eyes.

\subsection{Inherent Inaccuracy}\label{subsec:inherentinaccuracy}
%%main point: the inaccuracy of causality graph
%%\begin{itemize}
%% 	\item Paragraph1: describe the inaccuracy with general idea, define over
%%connection and under connection
%%
%%	
%%	\item Paragraph2: over connection patterns
%%
%%Over connections occur if intra-thread boundaries are missing from batch
%%processing programming paradigms. (dispatch\_mig\_service, runloop)
%%
%%	\item Paragraph3: under connection patterns
%%
%%Data dependencies inter/intra threads are usually hard to fully exploit in the
%%initial pass of graph computing. (shared flags in Object, data dependency for
%%delay work intra-thread)
%%
%%\end{itemize}

However, to construct an accurate and complete dependency graph is difficult,
if not impossible. The graph is inherently inaccurate. That one thread wakes
up the other thread does not always stand for a causility between them. In
implementation, \xxx filters out some of the definitive noise in the following
types.

%%We identify the purpose of wake-up and add huristics to filter out the false edges.

\begin{itemize}

\item interrupt processing and kernel sharetime maintanance that take over
current thread context.

\item timer expiration in the kernel which clears up all the waiting threads on
an event source.

\end{itemize}

In addition to the known definitive noise, there exist false connections and
miss data dependencies based on our experience while building dependency graph
with traditional causality tracing. We define them as over connection and under
connection respectively.

Over connections occur if intra-thread boundaries are missing from batch
processing programming paradigms. (dispatch\_mig\_service, runloop)

\para{Dispatch message batching}
The message dispatch service dequeues messages from many processes and staggers
processing of the messages. This creates false dependencies between each message
in the dispatch queue. As illustrated in the following code snipped from the
\vv{fontd} daemon, function \vv{dispatch\_execute} is installed as a callback to
a dispatch queue. It subsequently calls \vv{dispatch\_mig\_server()} which runs
the typical server loop and handles many messages.

To avoid incorrectly linking many irrelevant processes through such batching
processing patterns, \xxx adopts the aforementioned heuristics to split an
execution segment when it observes that the segment sends out messages to two
distinct processes. This pattern does pose a challenge for automated causal
tracing tools that assume that the entire execution of a callback function is
on behalf of one request. The code shown uses a dispatch-queue callback, but
inside the callback, it does work on behalf of many different requests. Any
application or daemon can implement its own server loop this way, which makes it
fundamentally difficult to automatically infer event handling boundaries.

{\footnotesize \begin{verbatim}
Worker thread in fontd daemon:
dispatch_async(block)

Main thread in fontd daemon:
block = dispatch_queue.dequeue()
dispatch_execute(block)
  dispatch_mig_server()

dispatch_msg_server()
  for(;;)
    mach_msg(send_reply, recev_request)
    call_back()
    set_reply()
\end{verbatim}
}

\para{Runloop callbacks batch processing}
As is common in event driven programming, many methods can post a callback and
MacOS uses runloop as a common idiom to process callbacks. As shown in the
following step-by-step description of the MacOS runloop, an iteration of the
runloop does 10 different stages of processing, each of which may do work on
behalf of completely irrelevant requests. Since there are no obvious events
(\eg, a wait operation) to split the execution, \xxx uses instrumentation to
add beginning and ending points for MacOS runloops. In general, any application
or daemon can create its own version of the runloop, posing challenges for
automated inference of event processing boundaries.

% // another thread installs cb
% performSelector:onThread:withObject:waitUntilDone;

{\footnotesize \begin{verbatim}
Run loop sequence of events //developer.apple.com
1-3.Notify observers
4.Fire any non-port-based input sources
5.If a port-based input source is ready and waiting to fire,
    process the event immediately. Go to step 9.
6.Notify observers that the thread is about to sleep.
7.Put the thread to sleep until:
    //one of the following events occurs
    An event arrives for a port-based input source.
    A timer fires.
    The timeout value set for the run loop expires.
    The run loop is explicitly woken up.
8.Notify observers that the thread just woke up.
9.Process the pending event.
  If a user-defined timer fired,
    process the timer event
    restart the loop.
    Go to step 2.
  If an input source fired
    deliver the event.
  If the run loop was explicitly woken up, but not timed out,
    restart the loop. Go to step 2.
10.Notify observers that the run loop has exited.

\end{verbatim}
}

\para{Batching and data dependency in event processing}
The WindowServer MacOS system daemon contains an event loop which waits on
Mach messages. Conceptually, it processes a series of independent events from
different processes. However, to presumably save on kernel boundary crossings,
it uses a single system call to receive data and send data for an unrelated
event. This batch processing artificially makes many events appear dependent,
and we split the execution segments to maintain the independence of the events.

This case also illustrates a causal linkage caused by data dependency within
one thread. As the code shows, WindowServer saves the reply message in variable
\vv{\_gOutMsg} inside function \vv{CGXPostReplyMessage}. When it calls
\vv{CGXRunOneServicePass}, it sends out \vv{\_gOutMsg} if there is any pending
message. This data dependency needs to be captured in order to establish a
causal link between the handling of the previous request and the send of the
reply. Interestingly, it is an example of a data dependency within the same
thread. \xxx uses watch point registers to capture events on these data flags
and establish causal links between them.

{\footnotesize \begin{verbatim}
while() {
  CGXPostReplyMessage(msg) {
  // send _gOutMsg if it hasn't been sent
    push_out_message(_gOutMsg)
    _gOutMsg = msg
    _gOutMessagePending = 1
  }
  CGXRunOneServicePass() {
    if (_gOutMessagePending)
      mach_msg_overwrite(MSG_SEND | MSG_RECV, _gOutMsg)
    else
      mach_msg(MSG_RECV)
    ... // process received message
  }
}
\end{verbatim}
}

Under connections are result from particular programing paradigms and data
dependencies. Data dependencies inter/intra threads are usually hard to fully
exploit in the initial pass of graph computing. (shared flags in Object, data
dependency for delay work intra-thread)

\para{CoreAnimation shared flags}
A worker thread can set a field \vv{need\_display} inside a CoreAnimation object
whenever the object needs to be repainted. The main thread iterates over all
animation objects and reads this flag, rendering any such object.

This shared-memory communication creates a dependency between the main thread
and the worker so accesses to these field flags need to be tracked. However,
since each object has such a field flag, \xxx cannot afford to monitor each
using a watch point register. Instead, it uses instrumentation to modify the
CoreAnimation library to trace events on these flags.

{\footnotesize \begin{verbatim}
Worker thread that needs to update UI:
ObjCoreAnimation->need_display = 1

Main thread: 
traverse all CoreAnimationobjects
if (obj->need_display == 1)
  render(obj)

\end{verbatim}
}

\para{Spinning cursor shared flag}
Whenever the system determines that the main thread has hung for a certain
period, and the spinning beach ball should be displayed, a shared-memory flag
is set. Access to this flag is controlled via a lock, i.e. the lock is used for
mutual exclusion, and does not imply a happens before relationship. Thus, \xxx
captures accesses to these flags using watch-point registers to add causal edges
correctly.

{\footnotesize \begin{verbatim}
NSEvent thread:
CGEventCreateNextEvent() {
  if (sCGEventIsMainThreadSpinning == 0x0)
     if (sCGEventIsDispatchToMainThread == 0x1)
       CFRunLoopTimerCreateWithHandler{
         if (sCGEventIsDispatchToMainThread == 0x1)
           sCGEventIsMainThreadSpinning = 0x1
           CGSConnectionSetSpinning(0x1);
       }
}

Main thread
Convert1CGEvent(0x1);
if (sCGEventIsMainThreadSpinning == 0x1)
  CGSConnectionSetSpinning(0x0);
  sCGEventIsMainThreadSpinning = 0x0;
  sCGEventIsDispatchedToMainThread = 0x0;
\end{verbatim}
}

In addition, the spurious edge introduced by mutex lock is also a challenge in
debugging. The synchronization on mutex lock reflects one thread wakes up the
other thread. They can depend on each other like producer and comsumer, while
no causality exists in the case of contention for a shared resource. Making the
graph completely sound without user interaction is almost impossible given the
essential attribite of commericial operating system as a grey box.

\subsection{User Interactions}

%%Main point for the subsection: why we need user interaction, what the user can
%%do and how it helps
%% Para1 : why we need user interaction
%% Para2 : what the user can do
%% Para3 : how it helps causality tracing

As we mentioned above, to figure out all the over connections and under
connections before hand is almost impossible. Instead, users can find out the
descrepency on the dependency graph while checking the computing result. For
example, we noticed that two unrelated applications connects to one node in the
graph, which leads to the manifestation of kernel thread batch processing on
timers. Users can gradually inject their knowledge until the graph is reasonable
for following debugging.

Over connections in this kind can be eliminated by adding heuristics to the
process of graph computing. In addition, users can also binary instrument the
image with the APIs provided by \xxx to amending the missing boundaries. On
contrary, if users discover under connections due to missing data dependency,
they can make use of the \xxx's hardware breakpointer tool to monitor the data
and add rules to recompute the graph.

Like other causality tracing approaches, \xxx is a general framework and tested
on limited data set. Allowing user interaction makes the dependency graph more
practical and useful case by case.

\subsection{Graph Computing Algorithm}

In the section, we describe the algorithm \xxx used to generate an event graph.
The algrithm has two main steps: construct nodes with heuristics base on
boundary events, and generate edges from the connection events.

A node is a sequence of events derived from the execution of a task in a thread.
As is shown in Figure ~\ref{fig:alg-graphcomputing}, \xxx checks events per
thread and applies heuristics when a boundary event is encoutered. \xxx provides
5 default heuristics. 4 of them shares the idea of general cuasual tracing, 1
is used to work around unknown programming pargdigms, and more are expected
from users to improve graphs. We discuss them in the following paragraph.

As known noises, event sequences for interrupt processing and kernel
maintainance are removed from threads, as is shown in 1.a. The second heuristics
1.b treats a wait event as an end of node. Wait event usually indicates a thread
switches to other tasks, but it is not always true considering a thread may park
due to low thread priority. One of the examples in MacOS is the pause of worker
threads draining a low priority diapatch queue. Dispatch queues are FIFO queues
to which an application can submit tasks in the form of block objects. Wait
events should not divide the block objects, otherwise it may result in a missing
connection. To save the intergrity of block objects, \xxx GraphComputeAlgorithm
keeps a counter \vv{callout\_level} to mark if the currently checking event is
inside a block object. Only the begin and end of Dispatch\_invoke are served
as the boundary for the node, as is shown in the step 1.c. The hueristics 1.d
is also for a batch processing mechanism. Runloop is an event processing loop
that used to schedule work and coordinate the receipt of incoming events.
The begin and end of the work invocation are served as boundaries. The last
default heuristics 1.e is used to work around the problem that batch processing
programming paragdigms are hard to exploit completely, as listed in previous
subsection ~\ref{subsec:inherentinaccuracy}. \xxx makes use of mach messages
to avoid clustering multiple tasks due to unknown batch processing. It defines
\vv{IPC\_peer\_set} to compute the set of mach message receivers/senders for
every node. For every mach message, the algorithm checks whether its peer
process exists in \vv{IPC\_peer\_set}, and adds the message into the current
node if the condition is true or the set is empty. Otherwise it add a boundary
for current node and begin a new node for the mach\_message event.

Edges connect nodes, both intra-thread and iter-thread, with connection events.
\xxx walks through each connection event types to apply heuritics as follows.
First, the return from a wait operation causally depends on the wake-up
operation. An edge is defined from the wake-up event to the first event after
the wait returns. \xxx also add a weak edges from the wait event to the wake-up
event in 2.a. Mach message is the core of ipc mechanism implemented in kernel,
upon which higher level RPC are built. \xxx connects the sender and receiver
of a mach message. For messages that expect a reply, \xxx also connects the
receiver of the original message and the sender of the reply message in 2.b.
As discussed above, dispatch queue and runloop are popular batch processing
programming paradigms in MacOS, \xxx connects submissions of a task and
executions of the task, which are listed in 2.c and 2.d respectively. Similarly,
\xxx adds edge from a timer armed event to its triggered event in 2.e. Shared
flag can either be traced with binary instrument, such as \vv{need\_display}
flag for CoreAnimation in subsection ~\ref{subsect:inherentinaccuracy}, or with
breakpoint watcher command line tool provided by \xxx. Edges from the flag set
to its read are added as 2.f. Since share variables are hard to completely
exploited, and the causality can be complicated than the writer-reader pattern,
user interaction are expected to remedy event graphs.

Finally, the computing graph is generated and subject to the improvement with
more input heuristics.

\begin{figure}[tb]
\footnotesize\begin{verbatim}
Algorithm ComputeControlFlowGraph:
    Input: Heuristics set + parsed tracing events
    Output: Control flow graph
1. Create nodes per thread:
  set callout_level = 0
  1.a interrupt/kern_maintainance: remove_events
  1.b wait: end of node if callout_level == 0
  1.c if dispatch_callout begin, create a new node
      and increase callout_level
      if dispatch_callout end, complete current node
      and decrease callout_level
  1.d Runloop: divide every callout
  1.e mach_msg: divide IPC with different peers
  1.f Other heuristics added by user from the input set.
2. Add edges for connection events:
  2.a wake_up event and wait event
  	edge(wake_up, first_waken_event)
	weak_edge(wait, wake_up)
  2.b mach message
  	edge(sender, receiver)
    if message needs reply
       edge(receiver, reply_sender)
  2.c dispatch queue events
  	edge(work_enqueue, work_dequeue)
  2.d Runloop work submission
  	edge(work_submit, work_invoke)
  2.e timer events
  	edge(timer_armed, timer_fired)
  2.f shared variable
  	edge(set_variable, read_and_clear_variable)
  2.g Other heuristics added by user from the input set.
3. Return the Graph with Nodes and Edges
\end{verbatim}
    \caption{\xxx Compute Graph algorithm.}
    \label{fig:alg-graphcomputing}
\end{figure}
