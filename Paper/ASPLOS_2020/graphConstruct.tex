\section{Handling Inaccuracies}\label{sec:graphcomputing}

In this section, we first describe the basics of \xxx event graphs
(\S\ref{subsec:eventgraph}), and then discuss how \xxx mitigates
over-connections (\S\ref{subsec:fix-over}) and under-connections
(\S\ref{subsec:fix-over}) in them.

\subsection{Event Graph Basics}\label{subsec:eventgraph}

To construct event graphs, \xxx collects three categories of events in its
systems-wide event logs.  The first category contains semantic events,
such as system calls, call stacks collected when certain operations such
as macOS message operations are run, and user actions such as key presses.
These events indicate what the developer intents might be, and are stored
as contents in each vertex in the event graph.  They are primarily for
providing information to user during diagnosis and for finding similar
vertices (\S\ref{sec:overview}).

The second category of events are boundary events that mark the beginning
and ending of execution segments or vertices in the graph.  \xxx handles
common callback invocations such as \vv{dispatch\_invoke} and
\vv{runloop\_invoke} and mark their entry and return as boundaries.

The third category of events are for forming edges in the graph.  For
instance, an operation that installs a callback is connected to the
execution of the callback.  A message send is connected to a message
receive.  The arming of a timer is connected to the execution of the timer
callback.  A unique design in \xxx is to trace general wake-up and wait
operations inside the kernel to ensure coverage across many diverse
user-level, possibly custom wake-up and wait operations because their
implementations almost always use kernel wake-up wand wait.  This approach
necessarily includes spurious edges in the graph, including those due to
mutual exclusion; \xxx handles them by querying the user when it
encounters a vertex with multiple incoming causal edges during diagnosis
(see \S\ref{sec:overview}).  We also observed that a waiting kernel thread
is frequently woken up to perform tasks such as interrupt handling and
scheduler maintenance; \xxx recognizes them and culls them out from the
graph automatically.

%% \xxx constructs dependency graphs with the events from tracing logs. Tracing
%% logs contain sequence of events per thread. Each event stands for an execution
%% step in a thread. They are grouped into nodes and IPCs, asynchronouns calls and
%% thread wakeups serve as edges; some edges can be inside a single thread.

%% The events traced in \xxx are carefully selected for three main purposes:
%% preserve semantics for the node, indentify node boundaries inside a thread,
%% and provide connections between nodes. We classify them into three categories:
%% semantics events, boundary events and connection events, as listed in
%% Table~\ref{table:event_types}.

%% \begin{table}[ht]
%% \begin{threeparttable}
%%   \centering
%%   \begin{tabularx}{\columnwidth}{|X|X|}
%%   	\hline
%%     \textbf{Event Type} & \textbf{Event Categories}\\
%% 	\hline
%% 	\hline
%% 		System\_call & Semantics\\ \hline
%% 		Back\_trace & Semantics\\ \hline
%% 		NSApp\_event\tnote{1} & Semantics \\ \hline
%% 		Wait & Semantics, Boundary \\ \hline
%% 		Interrupts & Boundary \\ \hline
%% 		Sharetime\_maintenance\tnote{2} & Boundary \\ \hline
%% 		Dispatch\_invoke\tnote{3} & Boundary \\ \hline
%% 		Runloop\_invoke & Boundary \\ \hline
%% 		Mach\_message & Boundary, Connection\\ \hline
%% 		Wake\_up & Connection \\ \hline
%% 		Timer & Connection \\ \hline
%% 		Dispatch\_enqueue & Connection \\ \hline
%% 		Runloop\_submit & Connection \\ \hline
%% 		Share\_flag\_read & Connection \\ \hline
%% 		Share\_flag\_write & Connection \\ \hline
%%   \end{tabularx}

%% 	\begin{tablenotes}
%% 		\footnotesize
%% 		\item [1] User input events dispatched to the Application.
%% 		\item [2] Kernel invoked a routine to update timeshare quota. 
%% 		\item [3] Invoke callback function for works from dispatch queue.
%% 	\end{tablenotes}
%%  \end{threeparttable}
%% \caption{Event Type Categories. }
%% \label{table:event_types}
%% \end{table}

Compared to tools such as \spindump that capture only the current
system state, event graphs capture the causal path of events, enabling
users to trace across threads and process to events happened in the past
(hence cannot be captured by \spindump) that explain present
anomalies.

%% Given the prevalent of multi-threading and multi-processing programs, bugs are
%% much more complicated. The long opening bugs are usually have several threads
%% involve, even across process boundaries. As an example, the always timeout on
%% particular synchronization primitive in one thread usually need to trace back to
%% find the other thread that was responsible for signal the primitive. Compared
%% to the existing debugging tools like lldb and spindump, the dependency graph is
%% useful in that 1) it provides thread relationships all over the system across
%% process boundary and timing boundary and 2) it records execution history for an
%% input event before users capture hangs with their eyes.


\subsection{Mitigating Over-Connections}\label{subsec:fix-over}

From a high-level, \xxx deals with over-connections by heuristically
splitting an execution segment that appears mixing handling of multiple
requests. It adds weak causal edges between the split segments in case
the splitting was incorrect. When a weak edge is encountered during
diagnosis, it queries users to decide whether to follow the weak edge
or stop (\S\ref{sec:overview}).

Specifically, \xxx splits based three criteria.  First, \xxx recognizes a
small set of well-known batch processing patterns such as
\vv{dispatch\_mig\_server()} in \S\ref{sec:inaccuracy} and splits the
batch into individual items.  Second, when a wait operation such as
\vv{recv()} blocks, \xxx splits the segment at the entry of the blocking
wait.  The rationale is that blocking wait is typically done at the last
during one step of event processing.  Third, if a segment communicates to
too many peering processes, \xxx splits the segment when the set of peers
differs.  Specifically, for each message, \xxx maintains a set of two
peers including (1) the direct sender or receiver of the message and (2)
the beneficiary of the message (macOS allows a process to send or receive
messages on behalf of a third process).  \xxx splits when two consecutive
message operations have non-overlapping peer sets.

\subsection{Mitigating Under-Connections}\label{subsec:fix-under}

Under-connections are primarily due to data dependencies.  Currently \xxx
queries the user to identify the memory locations of the data flags. It
is conceivable to leverage memory protection techniques to infer them
automatically, as demonstrated in previous record-replay
%% https://www.usenix.org/legacy/events/usenix05/tech/general/king/king.pdf
%% https://web.eecs.umich.edu/~pmchen/papers/dunlap08.pdf
work~\cite{king2005debugging, dunlap2008execution}, it is out of the scope
of this paper and we leave it for future work. Currently, to discover a data
flag, the user re-runs the application with \xxx to collect instruction traces
of the concurrent events in both the baseline and spinning cases and detects
where the control flow diverges. She then reruns the application with \xxx to
collect register values for the basic blocks before the divergence and uncovers
the address of the data flag. Once the user identifies a data flag, \xxx traces
it using either binary instrument, such as the \vv{need\_display} flag in
CoreAnimation (\S\ref{sec:inaccuracy}), or with watchpoints. \xxx add a causal
edge between a write to a data flag to the corresponding read to the flag.
For the known data flags mentioned above, \xxx traces them by default.
