\section{Argus Graph Computing}\label{sec:graphcomputing}

\subsection{Event Graph}\label{subsec:eventgraph}
%%Main point for the subsection: \xxx computes a dependency graph to assist
%%debugging. It is a useful graph to figure out relationships across thread
%%boundary and timing boundary 
%%\begin{itemize}
%%\item What the graph is?
%%	\begin{itemize}
%%	\item Paragraph1: high level description of dependency graph.
%%\xxx constructs dependency graphs with the events from tracing logs. Tracing
%%logs contains sequence of events per thread. Each event stands for an execution
%%step in the thread. They are grouped into nodes and IPCs, asynchronouns calls
%%and thread wakeups serve as edges; some edges can be inside a single thread.
%%	\item Paragraph2: detail about the graph.
%%Events can be classed into three categories: semantic, connection, boundary
%%Graph nodes consist of a list of execution events and edges are generated with
%%event pairs.
%%	\end{itemize}
%%\item Why the graph is userful?
%%The graph bares the causility path of a user input and thus is helpful in
%%debugging complicated performance bugs, which involve mutilple processes and
%%threads.
%%\end{itemize}

\xxx constructs dependency graphs with the events from tracing logs. Tracing
logs contain sequence of events per thread. Each event stands for an execution
step in a thread. They are grouped into nodes and IPCs, asynchronouns calls and
thread wakeups serve as edges; some edges can be inside a single thread.

The events traced in \xxx are carefully selected for three main purposes:
preserve semantics for the node, indentify node boundaries inside a thread,
and provide connections between nodes. We classify them into three categories:
semantics events, boundary events and connection events, as listed in
Table~\ref{table:event_types}.

\begin{table}[ht]
  \centering
  \begin{tabularx}{\columnwidth}{|X|X|}
  	\hline
    \textbf{Categories} & \textbf{Event types}\\
	\hline
	\hline
    {\bf Semantics Events} & System\_call\\
    provide hints for user & Back\_trace\\					   
	interactive debugging  & NSApp\_event\\
    \hline
    {\bf Boundary Events} & Interrupts\\
	construct nodes & Sharetime\_maintenance\\
	& Wait\\
	& Dispatch\_invoke\\
    & Runloop\_invoke\\
	& Mach\_message\\
    \hline
	{\bf Connection Events} & Wake\_up\\
    add edges & Mach\_message\\
    & Dispatch\_enqueue\\
    & Runloop\_submit\\
	& Share\_flag\_write\\
	& Share\_flag\_read\\
    \hline
  \end{tabularx}
  \caption{Event Type Categories. }
  \label{table:event_types}
\end{table}

Given the prevalent of multi-threading and multi-processing programs, bugs are
much more complicated. The long opening bugs are usually have several threads
involve, even across process boundaries. As an example, the always timeout on
particular synchronization primitive in one thread usually need to trace back to
find the other thread that was responsible for signal the primitive. Compared
to the existing debugging tools like lldb and spindump, the dependency graph is
useful in that 1) it provides thread relationships all over the system across
process boundary and timing boundary and 2) it records execution history for an
input event before users capture hangs with their eyes.

\subsection{Graph Computing Algorithm}

In the section, we describe the algorithm \xxx uses to generate an event graph.
The algrithm has two main steps: construct nodes with heuristics based on
boundary events, and generate edges from the connection events.

A node is a sequence of events derived from the execution of a task in a thread.
As shown in Algorithm ~\ref{alg:graphcomputing}, \xxx checks events per
thread and applies heuristics when a boundary event is checked. \xxx provides
five default heuristics. Four of them share the idea of general cuasual tracing,
one is used to work around unknown programming paradigms, and more are expected
from users to improve graphs. We discuss them in below.

As listed in (\S\ref{subsec:inherentinaccuracy}), event sequences
for interrupt processing and kernel maintainance are removed from
threads in line ~\ref{graphcomputing:1.a}. The second heuristics in line
~\ref{graphcomputing:1.b} treats a wait event as an end of a node. An wait event
usually indicates a thread switches to other tasks, but it is not always true
considering a thread may park due to low thread priority. One of the examples
in MacOS is the pause of worker threads draining a low priority diapatch queue.
Dispatch queues are FIFO queues to which an application can submit tasks in
the form of block objects. Wait events should not divide the block objects,
otherwise it may result in a missing connection. To save the intergrity of block
objects, \xxx keeps a counter \vv{callout\_level} to mark if the current event
is inside a block object. Only the begin and end of Dispatch\_invoke are served
as the boundary for the node, as is shown in line~\ref{graphcomputing:1.c}.
The hueristics also applies to runloop. Runloop is an event processing loop
that used to schedule work and coordinate the receipt of incoming events.
The begin and end of the work invocation are served as boundaries. The last
default heuristics in line~\ref{graphcomputing:1.e} is used to work around the
difficulty of exploiting batch processing programming paradigms completely, as
listed in follows (\S\ref{subsec:inherentinaccuracy}). \xxx makes use of mach
messages to avoid clustering multiple tasks due to unknown batch processing. It
defines \vv{IPC\_peer\_set} to compute the set of mach message receivers/senders
for every node. For every mach message, the algorithm checks whether its peer
process exists in \vv{IPC\_peer\_set}, and adds the message into the current
node if the condition is true or the set is empty. Otherwise it adds an
ending boundary for current node and constructs a new node beginning with the
mach\_message event.

Edges connect nodes, both intra-thread and inter-thread, with connection
events. \xxx walks through each connection event type to apply heuritics as
follows. First, the return from a wait operation causally depends on the wake-up
operation. An edge is defined from the wake-up event to the first event after
the wait returns. \xxx also add a weak edges from the wait event to the wake-up
event in line ~\ref{graphcomputing:2.a}. Mach message is the core of ipc
mechanism implemented in kernel, upon which higher level RPC are built. \xxx
connects the sender and receiver of a mach message. For messages that expect a
reply, \xxx also connects the receiver of the original message and the sender
of the reply message in line ~\ref{graphcomputing:2.b}. As discussed above,
dispatch queue and runloop are popular batch processing programming paradigms
in MacOS, \xxx connects submissions of a task and executions of the task, which
are listed in line ~\ref{graphcomputing:2.c} and line~\ref{graphcomputing:2.d}
respectively. Similarly, \xxx adds edge from a timer armed event to its
triggered event in line ~\ref{graphcomputing:2.e}. Shared flag can either be
traced with binary instrument, such as \vv{need\_display} flag for CoreAnimation
in (\S\ref{subsec:inherentinaccuracy}), or with breakpoint watcher command line
tool provided by \xxx. Edges from the flag written to its read are added from
line ~\ref{graphcomputing:2.f}. Since share variables are hard to completely
exploit, and the causality can be more complicated than the writer-reader
pattern, user interaction is expected.

Finally, a event graph returns and is subject to the improvement with
more user input heuristics.

\begin{algorithm}[ht!]
\caption{\xxx Compute Graph algorithm.}
\label{alg:graphcomputing}
\begin{algorithmic}[1]
\Require{Heuristics set + parsed tracing events}
\Ensure{Control flow graph}
\Statex
\Function{ComputeGraph}{}
\State {$callout\_level$ $\gets$ $0$}
\State {$IPC\_peer\_set$ $\gets$ $\{\}$}
\State {$current\_node$ $\gets$ $New Node$}
\For{Event: Events in a thread}
    \Switch {EventType}
	\Case {Interrupt or Timeshare\_maintenance} \label{graphcomputing:1.a}
		\State {Remove following events before return}
	\EndCase
	\Case {Wait} \label{graphcomputing:1.b}
		\If {$callout\_level$ $equals$ {$0$}} 
			\State {Add end boundary for current node}
            \State {$current\_node$ $\gets$ $New Node$}
		\EndIf
	\EndCase
	\Case {Dispatch\_invoke or Runloop\_invoke} \label{graphcomputing:1.c}
	 	\State {Divide each callout into a node}
	\EndCase
	\Case {Mach\_message}\label{graphcomputing:1.d}
		\If {$IPC\_peer\_set$ $\neq$ $\emptyset$ and peer $\notin$ $IPC\_peer\_set$} 
			\State {Add end boundary for current node}
        	\State {$current\_node$ $\gets$ $New Node$}
		\EndIf
		\State {update $IPC\_peer\_set$}
	\EndCase
	\EndSwitch
	\State {Other Heuristics}\label{graphcomputing:1.e}
\EndFor
\For{Event: Connection Events}
	\Switch {EventType}
	\Case {Wake\_up}\label{graphcomputing:2.a}
		\State{AddWeakEdge(corresponding wait, wake\_up)}
		\State{AddEdge(wake\_up, first event after wait returns)}
	\EndCase
	\Case {Mach\_message}\label{graphcomputing:2.b}
		\State{AddEdge(sender, receiver)}
		\If {needs reply}
			\State {AddEdge(receiver, reply sender)}
		\EndIf
	\EndCase
	\Case {Dispatch\_enqueue}\label{graphcomputing:2.c}
		\State{AddEdge(Dispatch\_enqueue, Dispatch\_invoke)}
	\EndCase
	\Case {Runloop\_submit}\label{graphcomputing:2.d}
		\State {AddEdge(Runloop\_submit, RunLoop\_invoke)}
	\EndCase
	\Case {Timer}\label{graphcomputing:2.e}
		\State {AddEdge(Timer armed, Timer callout)}
	\EndCase
	\Case {Share\_flag\_read}\label{graphcomputing:2.f}
		\State {AddEdge(Share\_flag\_write, Share\_flag\_read)}
	\EndCase
	\EndSwitch
	\State {Other Heuristics}
\EndFor
\EndFunction
\end{algorithmic}
\end{algorithm}

%%\begin{figure}[tb]
%%\footnotesize\begin{verbatim}
%%Algorithm ComputeControlFlowGraph:
%%    Input: Heuristics set + parsed tracing events
%%    Output: Control flow graph
%%1. Create nodes per thread:
%%  set callout_level = 0
%%  set IPC_peer_set = 0
%%  1.a interrupt/kern_maintainance: remove_events
%%  1.b wait: end of node if callout_level == 0
%%  1.c if dispatch_callout begin, create a new node
%%      and increase callout_level
%%      if dispatch_callout end, complete current node
%%      and decrease callout_level
%%  1.d Runloop: divide every callout
%%  1.e mach_msg: divide IPC with different peers
%%  1.f Other heuristics added by user from the input set.
%%
%%2. Add edges for connection events:
%%  2.a wake_up event and wait event
%%  	edge(wake_up, first_waken_event)
%%	weak_edge(wait, wake_up)
%%  2.b mach message
%%  	edge(sender, receiver)
%%    if message needs reply
%%       edge(receiver, reply_sender)
%%  2.c dispatch queue events
%%  	edge(work_enqueue, work_dequeue)
%%  2.d Runloop work submission
%%  	edge(work_submit, work_invoke)
%%  2.e timer events
%%  	edge(timer_armed, timer_fired)
%%  2.f shared variable
%%  	edge(set_variable, read_and_clear_variable)
%%  2.g Other heuristics added by user from the input set.
%%3. Return the Graph with Nodes and Edges
%%\end{verbatim}
%%    \caption{\xxx Compute Graph algorithm.}
%%    \label{fig:alg-graphcomputing}
%%\end{figure}

\subsection{Inherent Inaccuracy}\label{subsec:inherentinaccuracy}
%%main point: the inaccuracy of causality graph
%%\begin{itemize}
%% 	\item Paragraph1: describe the inaccuracy with general idea, define over
%%connection and under connection
%%	\item Paragraph2: over connection patterns
%%Over connections occur if intra-thread boundaries are missing from batch
%%processing programming paradigms. (dispatch\_mig\_service, runloop)
%%	\item Paragraph3: under connection patterns
%%Data dependencies inter/intra threads are usually hard to fully exploit in the
%%initial pass of graph computing. (shared flags in Object, data dependency for
%%delay work intra-thread)
%%\end{itemize}

However, to construct an accurate and complete dependency graph is difficult,
if not impossible. The graph is inherently inaccurate. That one thread wakes
up the other thread does not always stand for a causility between them. In
implementation, \xxx filters out some of the definitive noise in the following
types.

\begin{itemize}

\item interrupt processing and kernel sharetime maintanance that take over
current thread context.

\item timer expiration in the kernel which clears up all the waiting threads on
an event source.

\end{itemize}

In addition to the known definitive noise, there exist false connections and
missing dependencies based on our experience of building a dependency graph
with traditional causality tracing. We define them as over connection and under
connection respectively.

\subsubsection{Over Connections}\hfill\\
Over connections usually occur when intra-thread boundaries are missing due to
unknown batch processing programming paradigms. We list the example patterns we
found below.

\para{Dispatch message batching}
While traditional causual tracing assumes the entire execution of a
callback function is on behalf of one request, we found some daemon
implements its service loop inside the callback function and creates false
dependencies. In the following code snippet from the \vv{fontd} daemon, function
\vv{dispatch\_execute} is installed as a callback to a work from dispatch queue.
It subsequently calls \vv{dispatch\_mig\_server()} which runs the typical server
loop and handles messages from different apps.

To avoid incorrectly linking many irrelevant processes through such batching
processing patterns, \xxx adopts the aforementioned heuristics to split an
execution segment when it observes that the segment sends out messages to two
distinct processes. Any capplication or daemon can implement its own server loop
this way, which makes it fundamentally difficult to automatically infer event
handling boundaries.

{\footnotesize\begin{BVerbatim}[baseline=c,numbers=left]
worker thread in fontd:
//enqueue a block
block = dispatch_mig_sevice;
dispatch_async(block);
\end{BVerbatim}
}
\quad
{\footnotesize\begin{BVerbatim}[baseline=c]
main thread in fontd:
//dequeue blocks
block = dequeue();
dispatch_execute(block);
\end{BVerbatim}
}
	
{\verbatimfont{\itshape\ttfamily}\footnotesize\begin{BVerbatim}
//implementation of dipatch_mig_server
dispatch_mig_server()
  for(;;) //batch processing
    mach_msg(send_reply, recv_request)
    call_back(recv_request)
    set_reply(send_reply)
\end{BVerbatim}
}
\bigskip

\para{Batching in event processing}
Message activities inside a system call are assumed to be related traditionally.
However, to presumably save on kernel boundary crossings, WindowServer MacOS
system daemon uses a single system call to receive data and send data for an
unrelated event from differnt processed in its event loop. This batch processing
artificially makes many events appear dependent. We split the execution segments
to maintain the independence of the events.

{\footnotesize \begin{verbatim}
//inside a single thread
while() {
  CGXPostReplyMessage(msg) {
  // send _gOutMsg if it hasn't been sent
    push_out_message(_gOutMsg)
    _gOutMsg = msg
    _gOutMessagePending = 1
  }
  CGXRunOneServicePass() {
    if (_gOutMessagePending)
      mach_msg_overwrite(MSG_SEND | MSG_RECV, _gOutMsg)
    else
      mach_msg(MSG_RECV)
    ... // process received message
  }
}
\end{verbatim}
}

\subsubsection{Under Connections}\hfill\\
On the other hand, under connections mostly result from missing data
dependencies. Data dependencies inter/intra threads are usually hard to fully
exploit in the initial pass of graph computing.

\para{Data dependency in event processing}
The case above also illustrates a causal linkage caused by data dependency
within one thread. As the code shows, WindowServer saves the reply message in
variable \vv{\_gOutMsg} inside function \vv{CGXPostReplyMessage}. When it calls
\vv{CGXRunOneServicePass}, it sends out \vv{\_gOutMsg} if there is any pending
message. \xxx uses watch point registers to capture events on \vv{\_gOutMsg} and
establish the causal link between the handling of the previous request and the
send of the reply.

\para{CoreAnimation shared flags}
A worker thread can set a field \vv{need\_display} inside a CoreAnimation
object whenever the object needs to be repainted. The main thread iterates over
all animation objects and reads this flag, rendering any such object. This
shared-memory communication creates a dependency between the main thread and the
worker so accesses to these field flags need to be tracked.
%%However, since each object has such a field flag, \xxx cannot afford to monitor
%%each using a watch point register. Instead, it uses instrumentation to modify
%%the CoreAnimation library to trace events on these flags.

{\footnotesize\begin{BVerbatim}[baseline=c,numbers=left]
Worker thread:
//needs to update UI:
obj->need_display = 1
\end{BVerbatim}
}
\quad
{\footnotesize\begin{BVerbatim}[baseline=c,numbers=left]
Main thread: 
//traverse all CA objects
if (obj->need_display == 1)
  render(obj)
\end{BVerbatim}
}
\bigskip

\para{Spinning cursor shared flag}
Whenever the system determines that the main thread has hung for a certain
period, and the spinning beach ball should be displayed, a shared-memory flag
is set. Access to the flag is controlled via a lock, i.e. the lock is used for
mutual exclusion, and does not imply a happens before relationship. Thus, \xxx
captures accesses to these flags using watch-point registers to add causal edges
correctly.

{\footnotesize \begin{verbatim}
NSEvent thread:
CGEventCreateNextEvent() {
  if (sCGEventIsMainThreadSpinning == 0x0)
     if (sCGEventIsDispatchToMainThread == 0x1)
       CFRunLoopTimerCreateWithHandler{
         if (sCGEventIsDispatchToMainThread == 0x1)
           sCGEventIsMainThreadSpinning = 0x1
           CGSConnectionSetSpinning(0x1);
       }
}

Main thread:
Convert1CGEvent(0x1);
if (sCGEventIsMainThreadSpinning == 0x1)
  CGSConnectionSetSpinning(0x0);
  sCGEventIsMainThreadSpinning = 0x0;
  sCGEventIsDispatchedToMainThread = 0x0;
\end{verbatim}
}

In addition to batching processing and data dependency, the spurious edge
introduced by mutex lock is also a challenge in debugging. The synchronization
on mutex lock reflects one thread wakes up the other thread. The scenario can
either be a producer-comsumer problem or merely mutualy exclusion. Making the
graph completely sound without user interaction is almost impossible given the
essential attribite of commericial operating system as a grey box.

\subsection{User Interactions}\label{subsec:userinteraction}

%%Main point for the subsection: why we need user interaction, what the user can
%%do and how it helps
%% Para1 : why we need user interaction
%% Para2 : what the user can do
%% Para3 : how it helps causality tracing

As we mentioned above, to figure out all over connections and under connections
before hand is almost impossible. Instead, users can find out the descrepency
in event graph after the initial graph computing. For example, we noticed that
two unrelated applications connects to one node in the graph, which leads to the
manifestation of kernel thread batch processing on timers. Like other causality
tracing approaches, \xxx is a general framework for MacOS and tested on limited
programing paradigms. Allowing user interaction makes its graph more practical
and useful case by case.

Users can gradually inject their knowledge to improve the graph in two ways.
They can either add add heuristics to the graph computing algorithm in Algorithm
~\ref{alg:graphcomputing}, or binary instrument images with the APIs provided by
\xxx to expand the boundary or connection event categories.
