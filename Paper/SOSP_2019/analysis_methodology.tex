\section{Analysis Methodology}
It is comman in previous work to construct dependency graph or critical paths
with nodes representing high level semantics. 
However, without user input schema and programing structures of applications.
it is hard to infer the semantics of nodes in \xxx.
In addition, collecting all the semantic information system-wide is expensive
and not necessary in most running time.
\xxx constructs the relationship graphs first, narrows down the problem into a finer range,
and configures the dynamic data collection with concret debugging,
to achieve the understanding of performance issues in semantics level.

\subsection{Graph Construction}
We first reveal the causalities among threads by connecting events in the defined schema.
It includes the mach\_msg IPC, asynchronous calls via dispath queue, timers,
asynchronoun programing styles in RunLoops and thread schedulings. 

Then we identify the boundaries of requests and decouple them from the same thread.
In addition to events defined in the schema, we also take advantage of heuristics to infer boundaries.
The heuristics is based on the truth that usualy a request from one application to the daemon rarely
has causality to the ones from other user applications.
As we notice that Apple developed voucher system to attribute resource usage for the situation
that one process sends message to another process on behalf of a third process. 
We correct the heuristic with the vouchers, and only allows the events communiting with
different applications with vouchers reside in one execution segment.

Further, we inspect the graph repeately to discover the violations in the graph.
The traditional assumption that an asynchronous call is for one task is not true in MacOS.
For example, the dispath\_mig\_service used to batch processing messages from different user applications,
can be packed into a closure and pushed to a task queue.
In the case, we divide the processin in disoatch\_mig\_service into an individual node.
We also expose the interface for users to inspect the graph.

Finally, we generate a directed graph with the execution segments mapped into nodes,
and the causalities mapped into edges.

\subsection{Path Slicing}
For the purpose of performance issue debugging, the paths related to the problematic node
is more worthy than the whole graph.
Although extacting a graph for individual user transaction is hard due to the overwheming batchings,
slicing a path backward from the graph is relatively easier.

The first thing is to identify the targeted node.
In \xxx, we carried out the path slicing for normal execution as the baseline for comparisons.
Targeted nodes may different for different bug patterns.
For the case that the problematic node get blocking, 
we need to identify the normal node in the same application execution stage as the problematic node,
and choose the node wakes up the normal node in time as a target node to perfrom path slicing on it.

Then we add the precedent node into the path. 
In case that one node has multiple incoming edges,
we both support users' interference to choose one node or a DFS algorithm on default.
%For example, if a thread sends a request to dispatch queue and picks a worker thread to process,
%it will be likely to wake up the latter thread too.
%Therefore, edges of thread scheduling and asynchronous task causality appear between the same pair of nodes. 

The one benefit of backward path is it helps to exclude false connections.
The node falsely consisting of two different requests would finally encounter branches in the path.
As we saved the causality information in the graph, it is not hard to discriminate them and make a decision.
                                                                                                                         
\subsection{Node Comparison}
The performance isssue caused by the busy processing in UI thread is quite straightforward to diagnoze with out tool.
Debugging the UI thread blocking on the contention of resource is much more difficult.
In this situation, our tool is required to recognize the corresponding node
which obtained the resource in its normal execution.

Node comparision algorithm helps to allieviate users from the burden of inspecting large logs.
We first normalize the nodes with selected events.
In our system, we exclude the interrupts from the comparison
since the number and type of interrupts are usually different from execution to exection.
For the events that connected to other events, we normalize it with a peer attribute
to record the process id of its connecting peer,
We also record the name of the system calls, message id carried in mach\_msg for corresponding events.
The comparison algorithm omits the repeating times of the same events,
by checking if one node contains all distinct events in the other node.

The above step only idenfity the similarity of nodes.
We also define the differential attribtes to distinguish the normal node and spinning node,
including the waiting time, execution time and system call return values.

\subsection{Path Comparison}
To figure out the root cause of the missing wakeup in spinning case,
we check the threads of every node from the sliced normal path,
and report the thread that either get blocked or never had a similar node appers again
within the timing contraint.

%\subsection{Validation of bug fix}
%With the root cause revealed, users can come up with a binary patch with our instrumentation tool by themselves if necessary.
%\xxx also provides a way to check how the fix works either by comparing to the anomaly extract in section \ref{subsec:rootcause}. Alternatively, the user can add the tracing points in the binary patch for verification.
%For the case that has a timeout, we can narrow the timeout to check if the time interval when the main thread is non-responsive get reduced.
