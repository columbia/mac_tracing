\section{Introduction} \label{sec:intro}
%why we need the tool
%Challenge of debug with current tools
%Difference btw our tool and Magpie, AppInsight.etc
%only lldb: directly apply to the whole execution is too slow
%	 infeasible to run 24X7 to capture the bug log
%only depandancy graph:
%	false positive: timeout(long waiting thread is waken up by timeout, the waker is not the root cause)
%	false negtive: bug can be caused by some missing execution, which would only appear in normal execution
%

%How to conqour the challenges
%trace sysmtem wide, can be added in library and system
%get the relationships of threads from the log
%compare spin and normal case to narrow down the range in log
%apply lldb on a small range of code to unveil root cause
%

Today's web and desktop applications are predominantly parallel or
distributed, making performance issues in them extremely difficult to
diagnose because the handling of an external request is often spread
across many threads, processes, and sometimes machines instead of in one
sequential execution segment.  To manually reconstruct this graph of
execution segments for debugging, developers have to sift through a
massive amount of log entries and potentially code of related application
components.  More often than not, developers give up and resort to
guessing the root causes, producing "fixes" that sometimes make the matter
worse.  For instance, a bug in the Chrome browser causes a spinning cursor
in MacOS when a user switches the input method to XX~\cite{bug-url}.  It
was first reported in 2000 XX time, and developers attempted to add timers
to work around the issue.  Unfortunately, the bug remained and the timers
added made diagnosis even harder.  The bug remains open for XXX years.

Prior work proposed \emph{Causal tracing}, a powerful technique to
construct request graphs (semi-)automatically~\cite{xxx}. It does so by
inferring (1) the beginning and ending boundaries of the execution
segments (vertexes in the graph) involved in handling a request; and (2)
the causality between the segments (edges) -- how a segment causes others
to do additional handling of the request.  Prior causal tracing systems
all assumed certain programming idioms to automate inference.  For
instance, if a segment sends a message, signals a condition variable, or
posts a task to a work queue, it wakes up additional execution segments,
and prior systems assume that wake-ups reflect causality.  Similarly, they
assume that the execution segment from the beginning of a callback
invocation to the end is entirely for handling the request that causes the
callback to be installed~\cite{}.  They are shown to be quite effective
aiding developers to understand complex application behaviors and debug
real-world performance issues in XXX.

Unfortunately, based on our own study and experience of building a causal
tracing system for commercial operating system MacOS, we found that modern
applications frequently violate these assumptions, rendering the request
graphs computed by causal tracing imprecise in several ways.  First, an
inferred segment may be larger than the actual event handling segment due
to batch processing.  Specifically, for performance, an application or its
underlying frameworks may bundle work on behalf of multiple requests
together without no clear boundaries between them.  For instance, Windows
server in MacOS sends a reply for a previous request and receives a
message for the current request using one system call SendRecvXXX
presumably to reduce user-kernel crossings.

Second, the graphs may be missing numerous causal edges.  For instance,
consider ad hoc synchronization~\cite{xxx} via shared-memory flags: a
thread may set ``\v{flag = 1}'' and wake up another thread waiting on
``\v{while(!flag);}'' to do additional work.  Although the number of these
flags may be small, they often express critical causality, and not tracing
them would lead to many missing edges in the request graph.  However,
without knowing where the flags reside in memory, a tool would have to
trace all shared-memory operations, incurring prohibitive overhead and
adding many superfluous edges to the request graph.

Third, many inferred edges may be superfluous because wake-ups do not
necessarily reflect causality.  Consider an \v{unlock()} operation waking
up an thread waiting in \v{lock()}.  This wake-up may be just a
happens-stance and the developer intent is only mutual exclusion.
However, the actual semantics of the code may also enforce a causal order
between the two operations.

We believe that, without detailed understanding of application semantics,
request graphs computed by causal tracing are \emph{inherently} imprecise
and both over-approximate and under-approximate the reality.  Although
developer annotations can help improve precision~\cite{}, modern
applications use more and more third-party libraries whose source code is
not available.  In the case of users debugging performance issues such as
a spinning cursor on her own laptop, the application's code is often not
available.  Given the frequent use of custom synchronizations, work
queues, and shared memory flags in modern applications, it is hopeless to
count on manual annotations to ensure precise capture of request graphs.

We present \xxx, a practical system for effectively debugging performance
issues in modern desktop applications despite the imprecision of causal
tracing.  We designed \xxx to be interactive, as a debugger should be, so
that its users can easily inspect current diagnostics and guide the next
steps of debugging to counter the inherent imprecision of causal tracing.
For instance, \xxx's request graph contains likely missing and superfluous
edges for users to view and confirm during debugging.

Moreover, \xxx enables users to dynamically control the granularity of
tracing using a couple of intuitive primitives.  It starts off with
always-on, lightweight, system-wide tracing.  When a user observes a
performance issue (\eg, a spinning cursor), she can inspect the current
graph \xxx computes, configure \xxx to perform fine-grained tracing (\eg,
logging call stacks and instruction streams) for events she deems
relevant, and constructs a more detailed graph for diagnosis.  \xxx also
supports comparison of request graphs of normal vs buggy executions
because the difference of the similarly imprecise graphs often reveal
precise root causes.

We implemented \xxx in MacOS, a widely used commercial operating system.
MacOS is closed-source, so are its common frameworks and many
applications. It therefore provide a true test of \xxx.  We addresses
numerous nuances of MacOS that complicates causal tracing, and built a
system-wide, low-overhead tracer.  Our evaluation using XXX real-world
spinning cursors shows that \xxx is fast (XX\% overhead) and effective
(XX).

This paper makes the following contributions: our conceptual realization
that causal tracing is inherently imprecise and that interactive causal
tracing is superior than prior work in debugging performance issues in
modern applications; our system \xxx that performs system-wide tracing in
MacOS with little overhead; and our results diagnosing real-world spinning
cursors and finding root causes for performance issues the remained open
for XXX years.

This paper is organized as follows.

%% Deadlocks and livelocks are known to be significant challenges in developing distributed systems.
%% A great deal of work has gone into formal analysis of call graphs to try to identify deadlocks.
%% In practice, when faced with code that deadlocks or may deadlock, many developers will simply add timeouts to all code that obtains locks,
%% converting a show-stopping hang into a program that will eventually unfreeze itself.
%% However, the root cause of the cyclic dependency may go unaddressed, leading to significant user-facing delays in applications.

%% It is difficult for developers to diagnose livelocks with typical tracing or debugging tools (like \texttt{DTrace} or \texttt{lldb})
%% in user-facing applications for a number of reasons.
%% First, many function calls happen asynchronously through events or inter-process communication, especially in graphical programs.
%% This means that control flow travels through the kernel, and the root cause can be far away from an observed hang.
%% Many processes and threads may be involved, and it is unclear which ones to target.
%% Secondly, there is a vast number of events being triggered constantly, and
%% sifting through all the innocuous events to find relevant ones is a Herculean task.
%% Deadlocks that can be deterministically triggered are challenging enough, but
%% this problem is exacerbated by livelock situations which by their nature involve the execution of a continuous stream of operations.

%% In this work, we present our system \textit{\sys} which collects detailed tracing information across all processes on a Mac system.
%% \sys collects relevant userspace and kernelspace events and messages, and automatically correlates them between processes.
%% We allow trace points to be inserted at arbitrary locations in each process,
%% allowing a developer to collect additional information as they hone in on the cause of a livelock or deadlock.

%% One important aspect of analyzing livelocks is that timeouts typically use real wall time,
%% and any performance overhead incurred by an analysis tool may cause timeouts to expire more quickly,
%% affecting program behaviour. Our framework incurs minimal overhead of XXX\% in large-scale tests.
%% We also present two case studies of \sys applied to real-world problems, including a livelock in Google Chrome.
%% We believe that \sys represents an important step forward in debugging livelocks in user applications.
