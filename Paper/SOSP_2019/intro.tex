\section{Introduction} \label{sec:intro}
%why we need the tool
%Challenge of debug with current tools
%Difference btw our tool and Magpie, AppInsight.etc
%only lldb: directly apply to the whole execution is too slow
%	 infeasible to run 24X7 to capture the bug log
%only depandancy graph:
%	false positive: timeout(long waiting thread is waken up by timeout, the waker is not the root cause)
%	false negtive: bug can be caused by some missing execution, which would only appear in normal execution
%

%How to conqour the challenges
%trace sysmtem wide, can be added in library and system
%get the relationships of threads from the log
%compare spin and normal case to narrow down the range in log
%apply lldb on a small range of code to unveil root cause
%

Today's web and desktop applications are predominantly parallel or
distributed, making performance issues in them extremely difficult to
diagnose because the handling of an external request is often spread
across many threads, processes, and asynchronous
%execution
contexts
%sometimes machines
instead of in one
sequential execution segment\cite{harter2012file}.  To manually reconstruct this graph of execution
segments for debugging, developers have to sift through a massive amount of log
entries and potentially code of related application
components~\cite{chen2002pinpoint}, \cite{zhao2016non}, \cite{xu2009detecting},
\cite{nagaraj2012structured}, \cite{yuan2012conservative}.  More often
than not, developers give up and resort to guessing the root cause, producing
``fixes'' that sometimes make the matter worse.  For instance, a bug in the
Chrome browser engine causes a spinning cursor in MacOS when a user switches
the input method to Chromium Bugs\cite{chromiumbugreport}.  It was first reported in 2012, and
developers attempted to add timeouts to work around the issue.  Unfortunately,
the bug has remained open for seven years and the timeouts obscured diagnosis
further.

Prior work proposed \emph{Causal tracing}, a powerful technique to construct
request graphs (semi-)automatically~\cite{zhang2013panappticon}. It does so by
inferring (1) the beginning and ending boundaries of the execution segments
(vertices in the graph) involved in handling a request; and (2) the causality
between the segments (edges)---how a segment causes others to do additional
handling of the request.  Prior causal tracing systems all assumed certain
programming idioms to automate inference.  For instance, if a segment sends a
message, signals a condition variable, or posts a task to a work queue, it
wakes up additional execution segments, and prior systems assume that wake-ups
reflect causality.  Similarly, they assume that the execution segment from the
beginning of a callback invocation to the end is entirely for handling the
request that causes the callback to be
installed~\cite{zhang2013panappticon},\cite{ravindranath2012appinsight}.  Causal
tracing is quite effective at aiding developers to understand complex
application behaviors and debug real-world performance issues.% in XXX.

Unfortunately, based on our own study and experience of building a causal
tracing system for the commercial operating system MacOS, we found that modern
applications frequently violate these assumptions. Hence, the request
graphs computed by causal tracing are imprecise in several ways.  First, an
inferred segment may be larger than the actual event handling segment due
to batch processing.  Specifically, for performance, an application or its
underlying frameworks may bundle together work on behalf of multiple requests
without no clear distinguishing boundaries.  For instance, WindowServer
in MacOS sends a reply for a previous request and receives a
message for the current request using one system call mach\_msg\_overwrite\_trap,
presumably to reduce user-kernel crossings.

Second, the graphs may be missing numerous causal edges.  For instance,
consider ad hoc synchronization~\cite{xiong2010ad}
via shared-memory flags: a
thread may set ``\v{flag = 1}'' and wake up another thread waiting on
``\v{while(!flag);}'' to do additional work.  Even within one thread,
the code may set a data variable derived from one request and later uses
it in another request (\eg, the buffer that hols the reply in the
preceding WindowServer example). Although the number of these
flags may be small, they often express critical causality, and not tracing
them would lead to many missing edges in the request graph.  However,
without knowing where the flags reside in memory, a tool would have to
trace all memory operations, incurring prohibitive overhead and
adding many superfluous edges to the request graph.

Third, in any case, many inferred edges may be superfluous because wake-ups do not
necessarily reflect causality.  Consider an \v{unlock()} operation waking
up an thread waiting in \v{lock()}.  This wake-up may be just a
happens-stance and the developer intent is only mutual exclusion.
However, the actual semantics of the code may also enforce a causal order
between the two operations.

We believe that, without detailed understanding of application semantics,
request graphs computed by causal tracing are \emph{inherently} imprecise and
both over- and under-approximate reality.  Although developer annotations can
help improve precision~\cite{barham2004using},\cite{reynolds2006pip}, modern
applications use more and more third-party libraries whose source code is not
available.  In the case of tech-savvy users debugging performance issues such
as a spinning (busy) mouse cursor on her own laptop, the application's code is
often not available.  Given the frequent use of custom synchronizations, work
queues, and data flags in modern applications, it is hopeless to count on
manual annotations to ensure precise capture of request graphs.

In this work, we present \xxx, a practical system for effectively debugging
performance issues in modern desktop applications despite the imprecision of
causal tracing.  \xxx's goal is not to construct a per-request causal
graph which requires extremely precise causal tracing.  Instead, it
captures an approximate event graph for a duration of the system execution
to aid diagnosis.
We designed \xxx to be interactive, as a debugger should rightly be,
so that its users can easily inspect current diagnostics and guide the next
steps of debugging to counter the inherent imprecision of causal tracing. For
instance, \xxx's event graph contains many inaccurate edges that represent
false dependencies, which the user can address in an interactive manner.
When debugging a performance issue using \xxx, a user need only make edges
relevant to the issue precise.  In other words, she can provide schematic
information on demand, as opposed to full manual schema upfront for all
involved applications and daemons~\cite{barham2004using}.

Moreover, \xxx enables users to dynamically control the granularity of
tracing using a number of intuitive primitives. The system begins by using
always-on, lightweight, system-wide tracing.  When a user observes a
performance issue (\eg, a spinning cursor), she can inspect the current
graph \xxx computes, configure \xxx to perform finer-grained tracing (\eg,
logging call stacks and instruction streams) for events she deems
relevant, and trigger the issue again to construct a more detailed graph for diagnosis.  \xxx also
supports interactive search over the event graphs that contain both normal
and buggy executions for diagnosis.

We implemented \xxx in MacOS, a widely used commercial operating system. MacOS
is closed-source, as are its common frameworks and many of its applications.
This environment therefore provides a true test of \xxx.  We address multiple
nuances of MacOS that complicate causal tracing, and built a system-wide,
low-overhead tracer.

We evaluated \xxx on \nbug real-world, open spinning-cursor issues in
widely used applications such as the Chromium browser engine and MacOS
System Preferences, Installer, and Notes.  The root causes of all \nbug
issues were previously unknown to us and, to a large extent, the
public. Our results show that \xxx is effective: it helped us find all
root causes of issues, including the Chromium issue that remained open for
seven years.  \xxx is also fast: its systems-wide tracing incurs only 1\%
CPU overhead overall.

This paper makes the following contributions: our conceptual realization
that causal tracing is inherently imprecise and that interactive causal
tracing is superior than prior work in debugging performance issues in
modern applications; our system \xxx that performs system-wide tracing in
MacOS with little overhead; and our results diagnosing real-world spinning (busy)
cursors and finding root causes for performance issues that have remained open
for seven years.

This paper is organized as follows.

%% Deadlocks and livelocks are known to be significant challenges in developing distributed systems.
%% A great deal of work has gone into formal analysis of call graphs to try to identify deadlocks.
%% In practice, when faced with code that deadlocks or may deadlock, many developers will simply add timeouts to all code that obtains locks,
%% converting a show-stopping hang into a program that will eventually unfreeze itself.
%% However, the root cause of the cyclic dependency may go unaddressed, leading to significant user-facing delays in applications.

%% It is difficult for developers to diagnose livelocks with typical tracing or debugging tools (like \texttt{DTrace} or \texttt{lldb})
%% in user-facing applications for a number of reasons.
%% First, many function calls happen asynchronously through events or inter-process communication, especially in graphical programs.
%% This means that control flow travels through the kernel, and the root cause can be far away from an observed hang.
%% Many processes and threads may be involved, and it is unclear which ones to target.
%% Secondly, there is a vast number of events being triggered constantly, and
%% sifting through all the innocuous events to find relevant ones is a Herculean task.
%% Deadlocks that can be deterministically triggered are challenging enough, but
%% this problem is exacerbated by livelock situations which by their nature involve the execution of a continuous stream of operations.

%% In this work, we present our system \textit{\sys} which collects detailed tracing information across all processes on a Mac system.
%% \sys collects relevant userspace and kernelspace events and messages, and automatically correlates them between processes.
%% We allow trace points to be inserted at arbitrary locations in each process,
%% allowing a developer to collect additional information as they hone in on the cause of a livelock or deadlock.

%% One important aspect of analyzing livelocks is that timeouts typically use real wall time,
%% and any performance overhead incurred by an analysis tool may cause timeouts to expire more quickly,
%% affecting program behaviour. Our framework incurs minimal overhead of XXX\% in large-scale tests.
%% We also present two case studies of \sys applied to real-world problems, including a livelock in Google Chrome.
%% We believe that \sys represents an important step forward in debugging livelocks in user applications.
